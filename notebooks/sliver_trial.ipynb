{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ce94b5-ccda-4a23-a2ac-c1c0d5314d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, row_number, current_timestamp, concat_ws, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f823f11-6790-4ab3-8b6f-56755511772c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dc18cd117830:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Read_Silver_Layer</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f19e01ae7d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName(\"Read_Silver_Layer\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioLocalAccessKey\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioLocalSecretKey123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "for jar in os.listdir(\"/home/jovyan/jars\"):\n",
    "    if jar.endswith(\".jar\"):\n",
    "        spark.sparkContext.addJar(f\"/home/jovyan/jars/{jar}\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db8e171-cb82-4b0e-b51d-33092d9ee806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 302010\n"
     ]
    }
   ],
   "source": [
    "raw_data = spark.read.parquet(\"s3a://etl-dag/bronze/data/retail_data_parquet/\",\n",
    "                            header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Total records: {raw_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947280d0-312d-4f9e-b5dd-cfd26d904351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_ID',\n",
       " 'Customer_ID',\n",
       " 'Name',\n",
       " 'Email',\n",
       " 'Phone',\n",
       " 'Address',\n",
       " 'City',\n",
       " 'Zipcode',\n",
       " 'Country',\n",
       " 'Age',\n",
       " 'Gender',\n",
       " 'Income',\n",
       " 'Customer_Segment',\n",
       " 'Date',\n",
       " 'Year',\n",
       " 'Month',\n",
       " 'Time',\n",
       " 'Total_Purchases',\n",
       " 'Amount',\n",
       " 'Total_Amount',\n",
       " 'Product_Category',\n",
       " 'Product_Brand',\n",
       " 'Product_Type',\n",
       " 'Feedback',\n",
       " 'Shipping_Method',\n",
       " 'Payment_Method',\n",
       " 'Order_Status',\n",
       " 'Ratings',\n",
       " 'products',\n",
       " 'State/Province']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550b3c5-5b78-4118-a9ac-9d6786faf908",
   "metadata": {},
   "source": [
    "## Lookup Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a922304-2303-4746-a40c-3cbf8c8550a5",
   "metadata": {},
   "source": [
    "### 1. Countries Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227aba1c-f1f1-40d0-9350-642e1de1f618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|Country_ID|Country_Name|\n",
      "+----------+------------+\n",
      "|         1|     Germany|\n",
      "|         2|         USA|\n",
      "|         3|          UK|\n",
      "|         4|      Canada|\n",
      "|         5|   Australia|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = raw_data.select(\"Country\").where(col(\"Country\").isNotNull()).distinct().rdd.flatMap(lambda row: row).collect()\n",
    "countries_with_id = [Row(Country_ID=i+1, Country_Name=country) for i, country in enumerate(countries)]\n",
    "\n",
    "countries_df = spark.createDataFrame(countries_with_id)\n",
    "countries_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743021b9-21b6-4db6-bf29-fb9d0f980a75",
   "metadata": {},
   "source": [
    "### 2. States/Provinces Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c4adc3-e61c-47c0-b100-3c575308a9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|State_ID|          State_Name|Country_ID|\n",
      "+--------+--------------------+----------+\n",
      "|       1|             Alberta|         4|\n",
      "|       2|             Arizona|         2|\n",
      "|       3|Australian Capita...|         5|\n",
      "|       4|   Baden-WÃ¼rttemberg|         1|\n",
      "|       5|             Bavaria|         1|\n",
      "|       6|              Berlin|         1|\n",
      "|       7|              Bremen|         1|\n",
      "|       8|    British Columbia|         4|\n",
      "|       9|          California|         2|\n",
      "|      10|            Colorado|         2|\n",
      "|      11|District of Columbia|         2|\n",
      "|      12|         East Sussex|         3|\n",
      "|      13|             England|         3|\n",
      "|      14|             England|         4|\n",
      "|      15|             England|         5|\n",
      "|      16|             Florida|         2|\n",
      "|      17|             Georgia|         2|\n",
      "|      18|             Hamburg|         1|\n",
      "|      19|               Hesse|         1|\n",
      "|      20|            Illinois|         2|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states_data = raw_data.select(\"State/Province\", \"Country\").dropna().distinct()\n",
    "states_data = states_data.join(\n",
    "    countries_df,\n",
    "    states_data[\"Country\"] == countries_df[\"Country_Name\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "states_provinces_df = states_data.select(\n",
    "    row_number().over(Window.orderBy(\"State/Province\")).alias(\"State_ID\"),\n",
    "    col(\"State/Province\").alias(\"State_Name\"),\n",
    "    col(\"Country_ID\")\n",
    ")\n",
    "\n",
    "states_provinces_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb665af1-9fa7-48fa-bacc-d6b98de417fd",
   "metadata": {},
   "source": [
    "### 3. Cities Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "145c4f5a-02a6-426a-88ef-a26a0f3b3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+\n",
      "|City_ID|     City_Name|State_ID|\n",
      "+-------+--------------+--------+\n",
      "|      1|      Adelaide|      54|\n",
      "|      2|   Albuquerque|      34|\n",
      "|      3|Albury-Wodonga|      36|\n",
      "|      4|     Arlington|      57|\n",
      "|      5|       Atlanta|      17|\n",
      "|      6|        Austin|      57|\n",
      "|      7|      Ballarat|      58|\n",
      "|      8|     Baltimore|      27|\n",
      "|      9|        Barrie|      46|\n",
      "|     10|       Belfast|      41|\n",
      "|     11|       Bendigo|      58|\n",
      "|     12|        Berlin|       6|\n",
      "|     13|     Bielefeld|      40|\n",
      "|     14|    Birmingham|      13|\n",
      "|     15|    Birmingham|      14|\n",
      "|     16|    Birmingham|      15|\n",
      "|     17|        Bochum|      40|\n",
      "|     18|          Bonn|      40|\n",
      "|     19|        Boston|      28|\n",
      "|     20|        Bremen|       7|\n",
      "+-------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_data = raw_data.select(\"City\", \"State/Province\").dropna().distinct()\n",
    "\n",
    "cities_data = cities_data.join(\n",
    "    states_provinces_df,\n",
    "    cities_data['State/Province'] == states_provinces_df['State_Name'],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "cities_df = cities_data.select(\n",
    "    row_number().over(Window.orderBy(\"City\")).alias(\"City_ID\"),\n",
    "    col(\"City\").alias(\"City_Name\"),\n",
    "    col(\"State_ID\")\n",
    ")\n",
    "\n",
    "cities_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba7805-5d81-4fa9-bc60-a417a833d5a9",
   "metadata": {},
   "source": [
    "### 4. Categories Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86923d47-75fc-44a4-bf2e-b0d9afaf03fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|Category_ID|Category_Name|\n",
      "+-----------+-------------+\n",
      "|          1|      Grocery|\n",
      "|          2|  Electronics|\n",
      "|          3|     Clothing|\n",
      "|          4|        Books|\n",
      "|          5|   Home Decor|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = raw_data.select(\"Product_Category\").dropna().distinct().rdd.flatMap(lambda row: row).collect()\n",
    "categories_with_id = [Row(Category_ID=i+1, Category_Name=category) for i, category in enumerate(categories)]\n",
    "\n",
    "categories_df = spark.createDataFrame(categories_with_id)\n",
    "categories_df.show(n=categories_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2623a06-ce02-4a50-bee6-b6d87c334444",
   "metadata": {},
   "source": [
    "### 5. Brands Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1671ed15-3fe7-40c5-9e44-d0478e68e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|Brand_ID|       Brand_Name|\n",
      "+--------+-----------------+\n",
      "|       1|             Nike|\n",
      "|       2|    HarperCollins|\n",
      "|       3|             Sony|\n",
      "|       4|        Whirepool|\n",
      "|       5|             Zara|\n",
      "|       6|         BlueStar|\n",
      "|       7|     Random House|\n",
      "|       8|          Samsung|\n",
      "|       9|        Coca-Cola|\n",
      "|      10|       Home Depot|\n",
      "|      11|            Pepsi|\n",
      "|      12|Bed Bath & Beyond|\n",
      "|      13|    Penguin Books|\n",
      "|      14|       Mitsubhisi|\n",
      "|      15|           Nestle|\n",
      "|      16|            Apple|\n",
      "|      17|           Adidas|\n",
      "|      18|             IKEA|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brands = raw_data.select(\"Product_Brand\").dropna().distinct().rdd.flatMap(lambda row: row).collect()\n",
    "brands_with_id = [Row(Brand_ID=i+1, Brand_Name=brand) for i, brand in enumerate(brands)]\n",
    "\n",
    "brands_df = spark.createDataFrame(brands_with_id)\n",
    "brands_df.show(n=brands_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e6925-5b6f-4d29-addb-75420ef7dad9",
   "metadata": {},
   "source": [
    "### 6. Product Types Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d501b631-4af9-43ad-ae56-1f354276f23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------+\n",
      "|Product_Type_ID|   Product_Type_Name|Category_ID|\n",
      "+---------------+--------------------+-----------+\n",
      "|              1|            Bathroom|          5|\n",
      "|              2|             Bedding|          5|\n",
      "|              3|         BlueStar AC|          2|\n",
      "|              4|          Children's|          4|\n",
      "|              5|           Chocolate|          1|\n",
      "|              6|              Coffee|          1|\n",
      "|              7|         Decorations|          5|\n",
      "|              8|               Dress|          3|\n",
      "|              9|             Fiction|          4|\n",
      "|             10|              Fridge|          2|\n",
      "|             11|           Furniture|          5|\n",
      "|             12|          Headphones|          2|\n",
      "|             13|              Jacket|          3|\n",
      "|             14|               Jeans|          3|\n",
      "|             15|               Juice|          1|\n",
      "|             16|             Kitchen|          5|\n",
      "|             17|              Laptop|          2|\n",
      "|             18|            Lighting|          5|\n",
      "|             19|          Literature|          4|\n",
      "|             20|Mitsubishi 1.5 To...|          2|\n",
      "+---------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_data = raw_data.select(\"Product_Type\", \"Product_Category\").dropna().distinct()\n",
    "products_data = products_data.join(categories_df, products_data['Product_Category'] == categories_df['Category_Name'], how=\"inner\")\n",
    "\n",
    "products_df = products_data.select(\n",
    "    row_number().over(Window.orderBy(\"Product_Type\")).alias(\"Product_Type_ID\"),\n",
    "    col(\"Product_Type\").alias(\"Product_Type_Name\"),\n",
    "    col(\"Category_ID\")\n",
    ")\n",
    "\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48004a3-f3d9-4d31-a481-9a4eb71ba0e1",
   "metadata": {},
   "source": [
    "### 7. Customer Segments Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6634c483-97bd-4424-afda-d29a2e2d87f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|Segment_ID|Segment_Name|\n",
      "+----------+------------+\n",
      "|         1|     Premium|\n",
      "|         2|     Regular|\n",
      "|         3|         New|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "segments = raw_data.select(\"Customer_Segment\").dropna().distinct().rdd.flatMap(lambda row: row).collect()\n",
    "segments_with_id = [Row(Segment_ID=i+1, Segment_Name=segment) for i, segment in enumerate(segments)]\n",
    "\n",
    "segments_df = spark.createDataFrame(segments_with_id)\n",
    "segments_df.show(n=segments_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f073a-4c1f-4c99-8d2a-22d7f444d6f3",
   "metadata": {},
   "source": [
    "### 8. Income Levels Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "536a9e9c-6bdf-4ede-900e-f9a6439b2b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|Income_Level_ID|Income_Level|\n",
      "+---------------+------------+\n",
      "|              1|        High|\n",
      "|              2|         Low|\n",
      "|              3|      Medium|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incomes = raw_data.select(\"Income\").dropna().distinct().rdd.flatMap(lambda row: row).collect()\n",
    "incomes_with_id = [Row(Income_Level_ID=i+1, Income_Level=income) for i, income in enumerate(incomes)]\n",
    "\n",
    "incomes_df = spark.createDataFrame(incomes_with_id)\n",
    "incomes_df.show(n=incomes_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb6cd8-578a-47ed-8dc0-4f60754d319a",
   "metadata": {},
   "source": [
    "### 9. Shipping Methods Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8640cfd-8ddc-4f94-8d17-48e3cd0d621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|Shipping_Method_ID|Method_Name|\n",
      "+------------------+-----------+\n",
      "|                 1|    Express|\n",
      "|                 2|   Standard|\n",
      "|                 3|   Same-Day|\n",
      "+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shippings = raw_data.select(\"Shipping_Method\").dropna().distinct().rdd.flatMap(lambda row: row).collect()\n",
    "shippings_with_id = [Row(Shipping_Method_ID=i+1, Method_Name=shipping) for i, shipping in enumerate(shippings)]\n",
    "\n",
    "shippings_df = spark.createDataFrame(shippings_with_id)\n",
    "shippings_df.show(n=shippings_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a4b7e-a06b-4cfb-9cab-3f337ce9594c",
   "metadata": {},
   "source": [
    "### 10. Payment Methods Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efa8b090-096f-4384-a6a2-5498ee43221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|Payment_Method_ID|Method_Name|\n",
      "+-----------------+-----------+\n",
      "|                1|Credit Card|\n",
      "|                2|     PayPal|\n",
      "|                3|       Cash|\n",
      "|                4| Debit Card|\n",
      "+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payments = raw_data.select(\"Payment_Method\").dropna().distinct().rdd.flatMap(lambda row: row).collect()\n",
    "payments_with_id = [Row(Payment_Method_ID=i+1, Method_Name=payment) for i, payment in enumerate(payments)]\n",
    "\n",
    "payments_df = spark.createDataFrame(payments_with_id)\n",
    "payments_df.show(n=payments_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a6d81-0502-4746-810a-a76c6c92eefb",
   "metadata": {},
   "source": [
    "### 11. Order Statuses Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d683e5a7-cc9c-4b00-ba00-7ea04d0f7771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|Status_ID|Status_Name|\n",
      "+---------+-----------+\n",
      "|        1|    Shipped|\n",
      "|        2| Processing|\n",
      "|        3|  Delivered|\n",
      "|        4|    Pending|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = raw_data.select(\"Order_Status\").dropna().distinct().rdd.flatMap(lambda row: row).collect()\n",
    "orders_with_id = [Row(Status_ID=i+1, Status_Name=order) for i, order in enumerate(orders)]\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_with_id)\n",
    "orders_df.show(n=orders_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a5e39-aeb9-4645-819a-ace61c1d6388",
   "metadata": {},
   "source": [
    "## Core Entity Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ecc869-e10b-430d-b51d-164ce05c4ebd",
   "metadata": {},
   "source": [
    "### 1. Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d871635-d4bf-4ffa-bb6c-ae940b84cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+-------------+---+------+---------------+-------------------+--------------------+--------------------+\n",
      "|Customer_ID|              Name|               Email|        Phone|Age|Gender|Income_Level_ID|Customer_Segment_ID|        Created_Date|        Updated_Date|\n",
      "+-----------+------------------+--------------------+-------------+---+------+---------------+-------------------+--------------------+--------------------+\n",
      "|      95631|      Brandon Soto|  Shannon6@gmail.com|4.609969429E9| 45|  Male|              1|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      15275|     Robert Lester|Christopher34@gma...| 1.56863877E9| 23|Female|              2|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      59319|      Thomas Brady|  Cheryl24@gmail.com|2.698304078E9| 63|  Male|              2|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      27206|      Karen Larson| Kenneth84@gmail.com|6.708471497E9| 30|  Male|              2|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      22761|      Shelby Ramos|  Monica96@gmail.com|6.324744775E9| 35|  Male|              3|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      48976|   Dennis Alvarado|  Justin45@gmail.com| 1.29226441E9| 70|  Male|              3|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      76453|        Lori Marsh|  Nicole74@gmail.com|4.239985717E9| 70|  Male|              3|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      20305|      Jeffrey Mack| Michael31@gmail.com|8.011761879E9| 37|  Male|              3|                  1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      80782|   Carol Patterson| Cynthia73@gmail.com|2.488525816E9| 29|Female|              1|                  2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      46826|   Alicia Martinez|  Andrew18@gmail.com|2.697229682E9| 54|  Male|              1|                  2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      56508|   Duane Schroeder|  Daniel29@gmail.com|4.413527067E9| 67|Female|              2|                  2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      91199|        Lee Arnold|    Dana32@gmail.com|7.089158773E9| 29|  Male|              2|                  2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      80715|         Eric King|  Latoya86@gmail.com|2.624999684E9| 64|  Male|              3|                  2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      23481|         Jay Hodge|   Scott64@gmail.com|5.928492479E9| 28|Female|              1|                  3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      41457| Gregory Davis PhD|   Diane41@gmail.com|5.985393638E9| 35|  Male|              2|                  3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      36506|   Chelsea Maxwell|   Penny86@gmail.com|7.147279731E9| 37|Female|              2|                  3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      54882|Meredith Fernandez|    Kelly9@gmail.com|1.061797872E9| 25|Female|              2|                  3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      77263|      Jose Johnson|   Janet24@gmail.com|  8.7910725E9| 65|  Male|              3|                  3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      56342|  Kenneth Cisneros|  Robert25@gmail.com|4.580407057E9| 18|  Male|              3|                  3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      91063|        John Wolfe|     Joy81@gmail.com|3.560945688E9| 57|  Male|              3|                  3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "+-----------+------------------+--------------------+-------------+---+------+---------------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_base = raw_data.select('Customer_ID', 'Name', 'Email', 'Phone', 'Age','Gender', 'Income', 'Customer_Segment')\n",
    "\n",
    "customers_base = customers_base.join(incomes_df, customers_base[\"Income\"] == incomes_df[\"Income_Level\"], how=\"left\")\n",
    "customers_base = customers_base.join(segments_df, customers_base[\"Customer_Segment\"] == segments_df[\"Segment_Name\"], how=\"left\")\n",
    "\n",
    "customers_base.select('Customer_ID').dropna().rdd.flatMap(lambda row: row).collect()\n",
    "\n",
    "customers_df = customers_base.select(\n",
    "    col(\"Customer_ID\").cast(\"int\"),\n",
    "    col(\"Name\"),\n",
    "    col(\"Email\"),\n",
    "    col(\"Phone\").cast(\"string\"),\n",
    "    col(\"Age\").cast(\"int\"),\n",
    "    col(\"Gender\"),\n",
    "    col(\"Income_Level_ID\"),\n",
    "    col(\"Segment_ID\").alias(\"Customer_Segment_ID\"),\n",
    "    current_timestamp().alias(\"Created_Date\"),\n",
    "    current_timestamp().alias(\"Updated_Date\")\n",
    ")\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fe620-1bd1-4af6-ac9c-e5a6d026c53c",
   "metadata": {},
   "source": [
    "### 2. Products Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad33542a-f43b-4600-b07f-e78deb698f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+--------+-----------+--------------------+--------------------+----------+\n",
      "|Product_Name|Product_Type_ID|Brand_ID|Category_ID|        Created_Date|        Updated_Date|Product_ID|\n",
      "+------------+---------------+--------+-----------+--------------------+--------------------+----------+\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         1|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         2|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         3|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         4|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         5|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         6|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         7|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         8|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|         9|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        10|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        11|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        12|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        13|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        14|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        15|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        16|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        17|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        18|\n",
      "|       4K TV|             30|       8|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        19|\n",
      "|       4K TV|             30|       3|          2|2025-05-29 14:14:...|2025-05-29 14:14:...|        20|\n",
      "+------------+---------------+--------+-----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_base = raw_data.select('products', 'Product_Type','Product_Brand', 'Product_Category')\n",
    "\n",
    "products_base = products_base.join(products_df, products_base['Product_Type'] == products_df['Product_Type_Name'], how='left')\n",
    "products_base = products_base.join(brands_df, products_base['Product_Brand'] == brands_df['Brand_Name'], how='left')\n",
    "products_base = products_base.join(categories_df.select(\"Category_Name\"), products_base['Product_Category'] == categories_df['Category_Name'], how='left')\n",
    "\n",
    "products_df = products_base.select(\n",
    "    col(\"products\").alias(\"Product_Name\"),\n",
    "    col(\"Product_Type_ID\"),\n",
    "    col(\"Brand_ID\"),\n",
    "    col(\"Category_ID\"),\n",
    "    current_timestamp().alias(\"Created_Date\"),\n",
    "    current_timestamp().alias(\"Updated_Date\")\n",
    ").withColumn(\"Product_ID\", row_number().over(Window.orderBy(\"Product_Name\")))\n",
    "\n",
    "products_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b9a1e-e7fb-490b-968a-5d3d5fea463f",
   "metadata": {},
   "source": [
    "### 3. Addresses Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ffca552-2f2a-4681-85a9-70331d0a8c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------+-------+--------------------+--------------------+\n",
      "|Customer_ID|      Street_Address|City_ID|Zipcode|        Created_Date|        Updated_Date|\n",
      "+-----------+--------------------+-------+-------+--------------------+--------------------+\n",
      "|      22761|20065 Miller Junc...|    114|  33987|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      56342|26408 Kramer Over...|    162|  57214|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      15275|017 Rich Valleys ...|     12|  36597|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      27206|    40932 Anna Curve|    113|  32289|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      41457|    92812 Yates Fort|     22|  55112|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      80782|9115 Leah Haven A...|     16|  76288|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      80782|9115 Leah Haven A...|     15|  76288|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      80782|9115 Leah Haven A...|     14|  76288|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      54882|0033 Robertson Ho...|    147|  28666|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      91063|    47817 Allen Lock|     55|  15349|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      80715|2698 Fry Way Apt....|      7|  50500|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      91199|968 Orr Stravenue...|     51|  83910|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      48976|8870 Michael Cree...|     53|   8543|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      77263|5497 Stewart Stat...|      5|  42580|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      36506|4752 Jimmy Center...|    129|  48017|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      76453|286 Jennifer Prai...|      3|   7094|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      46826|   3348 Timothy Mews|     27|  35169|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      56508|94604 Samantha Cr...|     67|  95088|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      23481|  399 Charles Garden|    107|   5572|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|      23481|  399 Charles Garden|    106|   5572|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "+-----------+--------------------+-------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addresses_base = raw_data.select('Customer_ID', 'Address', 'City', 'State/Province', 'Zipcode', 'Country')\n",
    "addresses_base = addresses_base.join(cities_df, addresses_base['City'] == cities_df['City_Name'], how='left')\n",
    "\n",
    "addresses_df = addresses_base.select(\n",
    "    col(\"Customer_ID\").cast(\"int\"),\n",
    "    col(\"Address\").alias(\"Street_Address\"),\n",
    "    col(\"City_ID\"),\n",
    "    col(\"Zipcode\").cast(\"int\"),\n",
    "    current_timestamp().alias(\"Created_Date\"),\n",
    "    current_timestamp().alias(\"Updated_Date\")\n",
    ")\n",
    "\n",
    "addresses_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7883a-ca2d-46bb-bf7e-483550c627c0",
   "metadata": {},
   "source": [
    "### 4. Transactions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd4fe35f-a8f3-486f-b901-4636a5849ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------------------+------------+---------------+------------------+-----------------+---------------+--------------------+--------------------+\n",
      "|Transaction_ID|Customer_ID|Transaction_DateTime|Total_Amount|Total_Purchases|Shipping_Method_ID|Payment_Method_ID|Order_Status_ID|        Created_Date|        Updated_Date|\n",
      "+--------------+-----------+--------------------+------------+---------------+------------------+-----------------+---------------+--------------------+--------------------+\n",
      "|       5258286|      56342| 2025-05-29 21:39:43| 2248.229499|              5|                 1|                1|              1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       9245021|      80715| 2025-05-29 19:19:18| 597.4694199|              5|                 1|                1|              1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       3907436|      56508| 2025-05-29 13:55:19| 496.5221187|              6|                 2|                1|              1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       7851443|      54882| 2025-05-29 22:43:01| 2852.549556|              6|                 2|                4|              1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       7410980|      27206| 2025-05-29 12:09:27| 1512.225186|              6|                 2|                4|              1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       1593857|      95631| 2025-05-29 14:52:22| 868.2112112|              4|                 2|                3|              1|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       7665222|      80782| 2025-05-29 18:23:01| 2242.610338|              5|                 2|                4|              2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       2667919|      91063| 2025-05-29 21:53:21| 667.8658021|              8|                 2|                1|              2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       3977409|      48976| 2025-05-29 00:44:58| 1054.196198|              5|                 3|                2|              2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       3293656|      23481| 2025-05-29 09:03:52| 1714.869296|              9|                 3|                4|              2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       1927414|      20305| 2025-05-29 00:07:14|  103.807855|              3|                 3|                4|              2|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       9929212|      15275| 2025-05-29 22:32:16| 3037.596024|              8|                 1|                2|              3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       5188155|      59319| 2025-05-29 01:18:27| 3427.273821|             10|                 2|                3|              3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       3603052|      41457| 2025-05-29 21:33:11| 1717.856951|              7|                 2|                4|              3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       9615114|      36506| 2025-05-29 04:54:59| 1708.309266|              9|                 3|                2|              3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       1245123|      36347| 2025-05-29 02:08:45| 461.5204841|              4|                 3|                3|              3|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       2145094|      91199| 2025-05-29 20:14:49| 1509.789799|              8|                 1|                1|              4|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       9573957|      77263| 2025-05-29 13:21:11| 303.9043963|              2|                 2|                3|              4|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       8149573|      76453| 2025-05-29 16:19:42| 2208.141878|              8|                 2|                3|              4|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "|       4093755|      22761| 2025-05-29 19:23:20| 2273.081194|              6|                 3|                3|              4|2025-05-29 14:14:...|2025-05-29 14:14:...|\n",
      "+--------------+-----------+--------------------+------------+---------------+------------------+-----------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_base = raw_data\n",
    "\n",
    "transactions_base = transactions_base.join(shippings_df, transactions_base['Shipping_Method'] == shippings_df['Method_Name'], how='left')\n",
    "transactions_base = transactions_base.join(payments_df, transactions_base['Payment_Method'] == payments_df['Method_Name'], how='left')\n",
    "transactions_base = transactions_base.join(orders_df, transactions_base['Order_Status'] == orders_df['Status_Name'], how='left')\n",
    "\n",
    "transactions_df = transactions_base.select(\n",
    "    col('Transaction_ID').cast(\"int\"),\n",
    "    col(\"Customer_ID\").cast(\"int\"),\n",
    "    col('Time').alias(\"Transaction_DateTime\"),\n",
    "    col('Total_Amount'),\n",
    "    col(\"Total_Purchases\").cast(\"int\"),\n",
    "    col(\"Shipping_Method_ID\"),\n",
    "    col(\"Payment_Method_ID\"),\n",
    "    col(\"Status_ID\").alias(\"Order_Status_ID\"),\n",
    "    current_timestamp().alias(\"Created_Date\"),\n",
    "    current_timestamp().alias(\"Updated_Date\")\n",
    ")\n",
    "\n",
    "transactions_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
