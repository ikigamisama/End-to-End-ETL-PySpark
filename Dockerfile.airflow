FROM apache/airflow:3.0.1

USER root

# Install Java (required for PySpark to connect to Spark cluster)
RUN apt-get update && apt-get install -y default-jdk wget curl

# Set Java environment (will point to the default JDK installation)
ENV JAVA_HOME=/usr/lib/jvm/default-java

# Download and install Spark
RUN wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz && \
    tar -xzf spark-3.5.1-bin-hadoop3.tgz && \
    mv spark-3.5.1-bin-hadoop3 /opt/spark && \
    rm spark-3.5.1-bin-hadoop3.tgz && \
    chown -R airflow:root /opt/spark

# Add Hadoop AWS JARs for S3 connectivity
RUN mkdir -p /opt/spark-jars && \
    wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -P /opt/spark-jars/ && \
    wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar -P /opt/spark-jars/ && \
    chown -R airflow:root /opt/spark-jars

RUN mkdir -p /opt/spark/conf && \
    echo "spark.driver.extraClassPath /opt/spark-jars/*" > /opt/spark/conf/spark-defaults.conf && \
    echo "spark.executor.extraClassPath /opt/spark-jars/*" >> /opt/spark/conf/spark-defaults.conf && \
    echo "spark.sql.adaptive.enabled true" >> /opt/spark/conf/spark-defaults.conf && \
    echo "spark.sql.adaptive.coalescePartitions.enabled true" >> /opt/spark/conf/spark-defaults.conf && \
    chown -R airflow:root /opt/spark/conf

USER airflow

# Copy setup files
COPY requirements.txt /

# Install PySpark and other required packages along with requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt

# Set environment for Spark
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYSPARK_PYTHON=python3
ENV SPARK_CONF_DIR=$SPARK_HOME/conf
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH
ENV SPARK_LOCAL_IP=127.0.0.1

# Set additional Java options to avoid common issues
ENV PYSPARK_SUBMIT_ARGS="--driver-class-path /opt/spark-jars/* --jars /opt/spark-jars/* pyspark-shell"