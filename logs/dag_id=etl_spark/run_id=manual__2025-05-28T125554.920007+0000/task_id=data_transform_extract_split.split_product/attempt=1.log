{"timestamp":"2025-05-28T12:57:32.933824","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-28T12:57:32.934566","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/ETL_dags.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-28T12:57:33.607932","level":"info","event":"Secrets backends loaded for worker","count":1,"backend_classes":["EnvironmentVariablesBackend"],"logger":"supervisor"}
{"timestamp":"2025-05-28T12:57:33.826859","level":"warning","event":"Skipping masking for a secret as it's too short (<5 chars)","logger":"airflow.sdk.execution_time.secrets_masker"}
{"timestamp":"2025-05-28T12:57:33.827441","level":"info","event":"Connection Retrieved 'spark_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-05-28T12:57:33.829120","level":"info","event":"Spark-Submit cmd: spark-submit --master local[*] --conf spark.master=local[*] --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioLocalAccessKey --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.hadoop.fs.s3a.attempts.maximum=3 --conf spark.hadoop.fs.s3a.connection.establish.timeout=5000 --conf spark.hadoop.fs.s3a.connection.timeout=200000 --conf spark.hadoop.fs.s3a.buffer.dir=/tmp --conf spark.hadoop.fs.s3a.fast.upload=true --conf spark.hadoop.fs.s3a.fast.upload.buffer=disk --conf spark.hadoop.fs.s3a.multipart.size=104857600 --conf spark.hadoop.fs.s3a.multipart.threshold=2147483647 --driver-memory 2g --name split-product-local --verbose --deploy-mode client /opt/airflow/dags/spark_jobs/split_product.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.317867","level":"info","event":"Using properties file: /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.504794","level":"info","event":"Adding default property: spark.executor.extraClassPath=/opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.505013","level":"info","event":"Adding default property: spark.sql.adaptive.coalescePartitions.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.505117","level":"info","event":"Adding default property: spark.sql.adaptive.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.505278","level":"info","event":"Adding default property: spark.driver.extraClassPath=/opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.532115","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.532353","level":"info","event":"master                  local[*]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.532573","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.532695","level":"info","event":"deployMode              client","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.532776","level":"info","event":"executorMemory          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.532876","level":"info","event":"executorCores           null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533009","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533125","level":"info","event":"propertiesFile          /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533226","level":"info","event":"driverMemory            2g","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533323","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533424","level":"info","event":"driverExtraClassPath    /opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533572","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533680","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533778","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.533895","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.534008","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.534151","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.534281","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.534543","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.534744","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.534935","level":"info","event":"primaryResource         file:/opt/airflow/dags/spark_jobs/split_product.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.535053","level":"info","event":"name                    split-product-local","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.535213","level":"info","event":"childArgs               []","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.535321","level":"info","event":"jars                    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.535492","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.535615","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.535720","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.535837","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536004","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536109","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536229","level":"info","event":"--conf and those from the properties file /opt/spark/conf/spark-defaults.conf:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536337","level":"info","event":"(spark.driver.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536430","level":"info","event":"(spark.driver.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536522","level":"info","event":"(spark.executor.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536642","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536754","level":"info","event":"(spark.hadoop.fs.s3a.attempts.maximum,3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.536850","level":"info","event":"(spark.hadoop.fs.s3a.aws.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537018","level":"info","event":"(spark.hadoop.fs.s3a.buffer.dir,/tmp)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537124","level":"info","event":"(spark.hadoop.fs.s3a.connection.establish.timeout,5000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537277","level":"info","event":"(spark.hadoop.fs.s3a.connection.ssl.enabled,false)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537380","level":"info","event":"(spark.hadoop.fs.s3a.connection.timeout,200000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537494","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537630","level":"info","event":"(spark.hadoop.fs.s3a.fast.upload,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537769","level":"info","event":"(spark.hadoop.fs.s3a.fast.upload.buffer,disk)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.537898","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538004","level":"info","event":"(spark.hadoop.fs.s3a.multipart.size,104857600)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538100","level":"info","event":"(spark.hadoop.fs.s3a.multipart.threshold,2147483647)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538202","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538334","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538471","level":"info","event":"(spark.master,local[*])","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538603","level":"info","event":"(spark.sql.adaptive.coalescePartitions.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538729","level":"info","event":"(spark.sql.adaptive.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.538881","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:37.539125","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.053965","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.054268","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.054580","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.054733","level":"info","event":"file:/opt/airflow/dags/spark_jobs/split_product.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.054868","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.061114","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.061317","level":"info","event":"(spark.app.name,split-product-local)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.061490","level":"info","event":"(spark.app.submitTime,1748437058021)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.061635","level":"info","event":"(spark.driver.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.061806","level":"info","event":"(spark.driver.memory,2g)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.061957","level":"info","event":"(spark.executor.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.062117","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.062241","level":"info","event":"(spark.hadoop.fs.s3a.attempts.maximum,3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.062388","level":"info","event":"(spark.hadoop.fs.s3a.aws.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.062534","level":"info","event":"(spark.hadoop.fs.s3a.buffer.dir,/tmp)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.062656","level":"info","event":"(spark.hadoop.fs.s3a.connection.establish.timeout,5000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.062777","level":"info","event":"(spark.hadoop.fs.s3a.connection.ssl.enabled,false)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.062901","level":"info","event":"(spark.hadoop.fs.s3a.connection.timeout,200000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063023","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063145","level":"info","event":"(spark.hadoop.fs.s3a.fast.upload,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063270","level":"info","event":"(spark.hadoop.fs.s3a.fast.upload.buffer,disk)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063427","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063557","level":"info","event":"(spark.hadoop.fs.s3a.multipart.size,104857600)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063683","level":"info","event":"(spark.hadoop.fs.s3a.multipart.threshold,2147483647)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063811","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.063929","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.064559","level":"info","event":"(spark.master,local[*])","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.064780","level":"info","event":"(spark.sql.adaptive.coalescePartitions.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.064922","level":"info","event":"(spark.sql.adaptive.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.065055","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.065190","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.065935","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.066207","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.066444","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:38.066684","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.420922","level":"info","event":"25/05/28 12:57:40 INFO SparkContext: Running Spark version 3.5.1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.426214","level":"info","event":"25/05/28 12:57:40 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.426898","level":"info","event":"25/05/28 12:57:40 INFO SparkContext: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.544520","level":"info","event":"25/05/28 12:57:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.716894","level":"info","event":"25/05/28 12:57:40 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.718021","level":"info","event":"25/05/28 12:57:40 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.720528","level":"info","event":"25/05/28 12:57:40 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.720796","level":"info","event":"25/05/28 12:57:40 INFO SparkContext: Submitted application: split_product","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.871370","level":"info","event":"25/05/28 12:57:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.886973","level":"info","event":"25/05/28 12:57:40 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.888424","level":"info","event":"25/05/28 12:57:40 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.997887","level":"info","event":"25/05/28 12:57:40 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:40.999086","level":"info","event":"25/05/28 12:57:40 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.000611","level":"info","event":"25/05/28 12:57:40 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.002016","level":"info","event":"25/05/28 12:57:41 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.003010","level":"info","event":"25/05/28 12:57:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.453486","level":"info","event":"25/05/28 12:57:41 INFO Utils: Successfully started service 'sparkDriver' on port 33309.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.514955","level":"info","event":"25/05/28 12:57:41 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.582030","level":"info","event":"25/05/28 12:57:41 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.622880","level":"info","event":"25/05/28 12:57:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.625334","level":"info","event":"25/05/28 12:57:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.633354","level":"info","event":"25/05/28 12:57:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.693542","level":"info","event":"25/05/28 12:57:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a3c54f20-6ff9-4693-88b4-c70704b0e197","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.764753","level":"info","event":"25/05/28 12:57:41 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:41.825186","level":"info","event":"25/05/28 12:57:41 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.135645","level":"info","event":"25/05/28 12:57:42 INFO JettyUtils: Start Jetty 127.0.0.1:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.315894","level":"info","event":"25/05/28 12:57:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.332672","level":"info","event":"25/05/28 12:57:42 INFO Utils: Successfully started service 'SparkUI' on port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.594790","level":"info","event":"25/05/28 12:57:42 INFO Executor: Starting executor ID driver on host localhost","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.595170","level":"info","event":"25/05/28 12:57:42 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.596112","level":"info","event":"25/05/28 12:57:42 INFO Executor: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.612023","level":"info","event":"25/05/28 12:57:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/opt/spark-jars/*,file:/opt/airflow/*'","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.613396","level":"info","event":"25/05/28 12:57:42 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@259bf903 for default.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.649726","level":"info","event":"25/05/28 12:57:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37699.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.650585","level":"info","event":"25/05/28 12:57:42 INFO NettyBlockTransferService: Server created on localhost:37699","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.654643","level":"info","event":"25/05/28 12:57:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.667666","level":"info","event":"25/05/28 12:57:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 37699, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.675891","level":"info","event":"25/05/28 12:57:42 INFO BlockManagerMasterEndpoint: Registering block manager localhost:37699 with 1048.8 MiB RAM, BlockManagerId(driver, localhost, 37699, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.682573","level":"info","event":"25/05/28 12:57:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 37699, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:42.685810","level":"info","event":"25/05/28 12:57:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 37699, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:43.614436","level":"info","event":"25/05/28 12:57:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:43.631573","level":"info","event":"25/05/28 12:57:43 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:45.994569","level":"info","event":"25/05/28 12:57:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:46.022524","level":"info","event":"25/05/28 12:57:46 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:46.022813","level":"info","event":"25/05/28 12:57:46 INFO MetricsSystemImpl: s3a-file-system metrics system started","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:48.459820","level":"info","event":"25/05/28 12:57:48 INFO InMemoryFileIndex: It took 154 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:48.604288","level":"info","event":"25/05/28 12:57:48 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:52.256223","level":"info","event":"25/05/28 12:57:52 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:52.260248","level":"info","event":"25/05/28 12:57:52 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:52.936652","level":"info","event":"25/05/28 12:57:52 INFO CodeGenerator: Code generated in 276.557857 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.016215","level":"info","event":"25/05/28 12:57:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 211.5 KiB, free 1048.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.357341","level":"info","event":"25/05/28 12:57:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 1048.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.366916","level":"info","event":"25/05/28 12:57:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:37699 (size: 35.3 KiB, free: 1048.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.372450","level":"info","event":"25/05/28 12:57:53 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.389963","level":"info","event":"25/05/28 12:57:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7426254 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.619503","level":"info","event":"25/05/28 12:57:53 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.650500","level":"info","event":"25/05/28 12:57:53 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.651188","level":"info","event":"25/05/28 12:57:53 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.651935","level":"info","event":"25/05/28 12:57:53 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.654270","level":"info","event":"25/05/28 12:57:53 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.660872","level":"info","event":"25/05/28 12:57:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.768236","level":"info","event":"25/05/28 12:57:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 1048.5 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.771995","level":"info","event":"25/05/28 12:57:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 1048.5 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.774567","level":"info","event":"25/05/28 12:57:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:37699 (size: 6.4 KiB, free: 1048.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.775894","level":"info","event":"25/05/28 12:57:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.801834","level":"info","event":"25/05/28 12:57:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.803929","level":"info","event":"25/05/28 12:57:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.867468","level":"info","event":"25/05/28 12:57:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:53.887097","level":"info","event":"25/05/28 12:57:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.083910","level":"info","event":"25/05/28 12:57:54 INFO CodeGenerator: Code generated in 22.878136 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.090050","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 0-7426254, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.121488","level":"info","event":"25/05/28 12:57:54 INFO CodeGenerator: Code generated in 19.503724 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.297680","level":"info","event":"25/05/28 12:57:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1756 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.347933","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 496 ms on localhost (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.353316","level":"info","event":"25/05/28 12:57:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.360653","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.677 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.366749","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.368072","level":"info","event":"25/05/28 12:57:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.372111","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.751726 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.404963","level":"info","event":"25/05/28 12:57:54 INFO CodeGenerator: Code generated in 14.280897 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.542917","level":"info","event":"25/05/28 12:57:54 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.543801","level":"info","event":"25/05/28 12:57:54 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.556455","level":"info","event":"25/05/28 12:57:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 211.5 KiB, free 1048.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.594917","level":"info","event":"25/05/28 12:57:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 1048.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.637079","level":"info","event":"25/05/28 12:57:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:37699 (size: 35.3 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.646761","level":"info","event":"25/05/28 12:57:54 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.648984","level":"info","event":"25/05/28 12:57:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7426254 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.657935","level":"info","event":"25/05/28 12:57:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:37699 in memory (size: 6.4 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.773177","level":"info","event":"25/05/28 12:57:54 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.776680","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 12 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.777022","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.777195","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.777609","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.779916","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.827145","level":"info","event":"25/05/28 12:57:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 29.0 KiB, free 1048.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.835953","level":"info","event":"25/05/28 12:57:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 1048.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.836690","level":"info","event":"25/05/28 12:57:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:37699 (size: 13.4 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.837574","level":"info","event":"25/05/28 12:57:54 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.838723","level":"info","event":"25/05/28 12:57:54 INFO DAGScheduler: Submitting 12 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.838967","level":"info","event":"25/05/28 12:57:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 12 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.841558","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.842672","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (localhost, executor driver, partition 1, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.843534","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (localhost, executor driver, partition 2, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.844313","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (localhost, executor driver, partition 3, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.845091","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (localhost, executor driver, partition 4, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.845849","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (localhost, executor driver, partition 5, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.846579","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (localhost, executor driver, partition 6, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.847232","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (localhost, executor driver, partition 7, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.847928","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (localhost, executor driver, partition 8, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.848603","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (localhost, executor driver, partition 9, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.849427","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (localhost, executor driver, partition 10, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.850190","level":"info","event":"25/05/28 12:57:54 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (localhost, executor driver, partition 11, PROCESS_LOCAL, 8219 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.851313","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.851775","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.853195","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.858937","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.861092","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.865820","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.878320","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.879064","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.882015","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.883987","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.893921","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.899019","level":"info","event":"25/05/28 12:57:54 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.947302","level":"info","event":"25/05/28 12:57:54 INFO CodeGenerator: Code generated in 16.325899 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.952748","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 66836286-74262540, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.953284","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 81688794-84920745, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.960311","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 37131270-44557524, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.960537","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 74262540-81688794, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.960660","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 44557524-51983778, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.960765","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 59410032-66836286, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.960865","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 22278762-29705016, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.960962","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 29705016-37131270, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.961060","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 51983778-59410032, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.961223","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 0-7426254, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.962650","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 14852508-22278762, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:54.968632","level":"info","event":"25/05/28 12:57:54 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 7426254-14852508, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:57:55.984754","level":"info","event":"25/05/28 12:57:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:37699 in memory (size: 35.3 KiB, free: 1048.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:01.281019","level":"info","event":"25/05/28 12:58:01 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:01.325011","level":"info","event":"25/05/28 12:58:01 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 6475 ms on localhost (executor driver) (1/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.350674","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.367300","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7526 ms on localhost (executor driver) (2/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.371999","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.385016","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 1653 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.397030","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 7546 ms on localhost (executor driver) (3/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.404486","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 7558 ms on localhost (executor driver) (4/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.470144","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.473817","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 7632 ms on localhost (executor driver) (5/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.522406","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.524965","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1653 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.529654","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 7677 ms on localhost (executor driver) (6/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.539155","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 7696 ms on localhost (executor driver) (7/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.553663","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.556948","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 7713 ms on localhost (executor driver) (8/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.789480","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.795682","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 7947 ms on localhost (executor driver) (9/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.796028","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.805348","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 7957 ms on localhost (executor driver) (10/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.859651","level":"info","event":"25/05/28 12:58:02 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 1610 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:02.862132","level":"info","event":"25/05/28 12:58:02 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 8014 ms on localhost (executor driver) (11/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:03.164388","level":"info","event":"25/05/28 12:58:03 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 1653 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:03.171229","level":"info","event":"25/05/28 12:58:03 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8326 ms on localhost (executor driver) (12/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:03.171587","level":"info","event":"25/05/28 12:58:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:03.172613","level":"info","event":"25/05/28 12:58:03 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 8.390 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:03.173096","level":"info","event":"25/05/28 12:58:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:03.173540","level":"info","event":"25/05/28 12:58:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:03.180432","level":"info","event":"25/05/28 12:58:03 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 8.424350 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.282578","level":"info","event":"25/05/28 12:58:04 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.282863","level":"info","event":"25/05/28 12:58:04 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.624715","level":"info","event":"25/05/28 12:58:04 INFO CodeGenerator: Code generated in 158.643974 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.636145","level":"info","event":"25/05/28 12:58:04 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.4 KiB, free 1048.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.665542","level":"info","event":"25/05/28 12:58:04 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 1048.3 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.676713","level":"info","event":"25/05/28 12:58:04 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:37699 (size: 35.3 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.679030","level":"info","event":"25/05/28 12:58:04 INFO SparkContext: Created broadcast 4 from parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.695209","level":"info","event":"25/05/28 12:58:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7426254 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.816028","level":"info","event":"25/05/28 12:58:04 INFO DAGScheduler: Registering RDD 13 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.826129","level":"info","event":"25/05/28 12:58:04 INFO DAGScheduler: Got map stage job 2 (parquet at NativeMethodAccessorImpl.java:0) with 12 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.832377","level":"info","event":"25/05/28 12:58:04 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.834543","level":"info","event":"25/05/28 12:58:04 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.837513","level":"info","event":"25/05/28 12:58:04 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.843852","level":"info","event":"25/05/28 12:58:04 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.910642","level":"info","event":"25/05/28 12:58:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 42.9 KiB, free 1048.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.935556","level":"info","event":"25/05/28 12:58:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 1048.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.952890","level":"info","event":"25/05/28 12:58:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:37699 (size: 19.1 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.954592","level":"info","event":"25/05/28 12:58:04 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.963390","level":"info","event":"25/05/28 12:58:04 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.963789","level":"info","event":"25/05/28 12:58:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 12 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.966440","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 13) (localhost, executor driver, partition 0, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.967208","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 14) (localhost, executor driver, partition 1, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.969249","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 15) (localhost, executor driver, partition 2, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.969500","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 16) (localhost, executor driver, partition 3, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.969636","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 17) (localhost, executor driver, partition 4, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.974329","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 18) (localhost, executor driver, partition 5, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.983307","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 19) (localhost, executor driver, partition 6, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.992385","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 20) (localhost, executor driver, partition 7, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:04.998813","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 21) (localhost, executor driver, partition 8, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.000347","level":"info","event":"25/05/28 12:58:04 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 22) (localhost, executor driver, partition 9, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.001699","level":"info","event":"25/05/28 12:58:05 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 23) (localhost, executor driver, partition 10, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.003495","level":"info","event":"25/05/28 12:58:05 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 24) (localhost, executor driver, partition 11, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.006917","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 1.0 in stage 2.0 (TID 14)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.007141","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 0.0 in stage 2.0 (TID 13)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.016496","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 2.0 in stage 2.0 (TID 15)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.035641","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 4.0 in stage 2.0 (TID 17)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.060190","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 5.0 in stage 2.0 (TID 18)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.060394","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 3.0 in stage 2.0 (TID 16)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.072518","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 6.0 in stage 2.0 (TID 19)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.072836","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 7.0 in stage 2.0 (TID 20)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.072970","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 10.0 in stage 2.0 (TID 23)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.073068","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 8.0 in stage 2.0 (TID 21)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.073896","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 11.0 in stage 2.0 (TID 24)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.084660","level":"info","event":"25/05/28 12:58:05 INFO Executor: Running task 9.0 in stage 2.0 (TID 22)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.174403","level":"info","event":"25/05/28 12:58:05 INFO CodeGenerator: Code generated in 80.834605 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.228043","level":"info","event":"25/05/28 12:58:05 INFO CodeGenerator: Code generated in 28.970891 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.286285","level":"info","event":"25/05/28 12:58:05 INFO CodeGenerator: Code generated in 13.855443 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.426140","level":"info","event":"25/05/28 12:58:05 INFO CodeGenerator: Code generated in 92.059962 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.516892","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 51983778-59410032, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.537091","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 7426254-14852508, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.542694","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 22278762-29705016, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.586011","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 44557524-51983778, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.586302","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 14852508-22278762, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.586442","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 81688794-84920745, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.586559","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 29705016-37131270, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.586669","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 37131270-44557524, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.586807","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 59410032-66836286, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.592648","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 0-7426254, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.594417","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 74262540-81688794, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:05.604122","level":"info","event":"25/05/28 12:58:05 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 66836286-74262540, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:06.123072","level":"info","event":"25/05/28 12:58:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:37699 in memory (size: 35.3 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:06.233168","level":"info","event":"25/05/28 12:58:06 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:37699 in memory (size: 13.4 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:07.884545","level":"info","event":"25/05/28 12:58:07 INFO Executor: Finished task 11.0 in stage 2.0 (TID 24). 2735 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:07.963218","level":"info","event":"25/05/28 12:58:07 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 24) in 2960 ms on localhost (executor driver) (1/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.326910","level":"info","event":"25/05/28 12:58:08 INFO Executor: Finished task 10.0 in stage 2.0 (TID 23). 2735 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.407550","level":"info","event":"25/05/28 12:58:08 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 23) in 3406 ms on localhost (executor driver) (2/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.515820","level":"info","event":"25/05/28 12:58:08 INFO Executor: Finished task 3.0 in stage 2.0 (TID 16). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.535527","level":"info","event":"25/05/28 12:58:08 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 16) in 3564 ms on localhost (executor driver) (3/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.578281","level":"info","event":"25/05/28 12:58:08 INFO Executor: Finished task 7.0 in stage 2.0 (TID 20). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.598786","level":"info","event":"25/05/28 12:58:08 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 20) in 3606 ms on localhost (executor driver) (4/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.688434","level":"info","event":"25/05/28 12:58:08 INFO Executor: Finished task 4.0 in stage 2.0 (TID 17). 2735 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.696425","level":"info","event":"25/05/28 12:58:08 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 17) in 3727 ms on localhost (executor driver) (5/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.697181","level":"info","event":"25/05/28 12:58:08 INFO Executor: Finished task 9.0 in stage 2.0 (TID 22). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.714758","level":"info","event":"25/05/28 12:58:08 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 22) in 3715 ms on localhost (executor driver) (6/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.926653","level":"info","event":"25/05/28 12:58:08 INFO Executor: Finished task 5.0 in stage 2.0 (TID 18). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.946519","level":"info","event":"25/05/28 12:58:08 INFO Executor: Finished task 2.0 in stage 2.0 (TID 15). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.953802","level":"info","event":"25/05/28 12:58:08 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 18) in 3980 ms on localhost (executor driver) (7/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:08.973802","level":"info","event":"25/05/28 12:58:08 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 15) in 4005 ms on localhost (executor driver) (8/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.059164","level":"info","event":"25/05/28 12:58:09 INFO Executor: Finished task 8.0 in stage 2.0 (TID 21). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.066046","level":"info","event":"25/05/28 12:58:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 13). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.069179","level":"info","event":"25/05/28 12:58:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 13) in 4104 ms on localhost (executor driver) (9/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.070118","level":"info","event":"25/05/28 12:58:09 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 21) in 4072 ms on localhost (executor driver) (10/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.235351","level":"info","event":"25/05/28 12:58:09 INFO Executor: Finished task 6.0 in stage 2.0 (TID 19). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.239337","level":"info","event":"25/05/28 12:58:09 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 19) in 4259 ms on localhost (executor driver) (11/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.322998","level":"info","event":"25/05/28 12:58:09 INFO Executor: Finished task 1.0 in stage 2.0 (TID 14). 2692 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.328888","level":"info","event":"25/05/28 12:58:09 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 14) in 4362 ms on localhost (executor driver) (12/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.329087","level":"info","event":"25/05/28 12:58:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.341350","level":"info","event":"25/05/28 12:58:09 INFO DAGScheduler: ShuffleMapStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 4.475 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.341578","level":"info","event":"25/05/28 12:58:09 INFO DAGScheduler: looking for newly runnable stages","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.341730","level":"info","event":"25/05/28 12:58:09 INFO DAGScheduler: running: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.341869","level":"info","event":"25/05/28 12:58:09 INFO DAGScheduler: waiting: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.342028","level":"info","event":"25/05/28 12:58:09 INFO DAGScheduler: failed: Set()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:09.430243","level":"info","event":"25/05/28 12:58:09 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:10.946939","level":"info","event":"25/05/28 12:58:10 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:11.279177","level":"info","event":"25/05/28 12:58:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:11.288431","level":"info","event":"25/05/28 12:58:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:11.288940","level":"info","event":"25/05/28 12:58:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:11.289209","level":"info","event":"25/05/28 12:58:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:11.291126","level":"info","event":"25/05/28 12:58:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:11.291337","level":"info","event":"25/05/28 12:58:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:17.834275","level":"info","event":"25/05/28 12:58:17 INFO CodeGenerator: Code generated in 150.528947 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.006010","level":"info","event":"25/05/28 12:58:18 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.010175","level":"info","event":"25/05/28 12:58:18 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.010360","level":"info","event":"25/05/28 12:58:18 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.010487","level":"info","event":"25/05/28 12:58:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.011048","level":"info","event":"25/05/28 12:58:18 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.011888","level":"info","event":"25/05/28 12:58:18 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.120164","level":"info","event":"25/05/28 12:58:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 255.1 KiB, free 1048.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.127643","level":"info","event":"25/05/28 12:58:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 93.6 KiB, free 1048.2 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.129132","level":"info","event":"25/05/28 12:58:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37699 (size: 93.6 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.130746","level":"info","event":"25/05/28 12:58:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.133931","level":"info","event":"25/05/28 12:58:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.134212","level":"info","event":"25/05/28 12:58:18 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.145700","level":"info","event":"25/05/28 12:58:18 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 25) (localhost, executor driver, partition 0, NODE_LOCAL, 7615 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.147906","level":"info","event":"25/05/28 12:58:18 INFO Executor: Running task 0.0 in stage 4.0 (TID 25)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.313128","level":"info","event":"25/05/28 12:58:18 INFO ShuffleBlockFetcherIterator: Getting 12 (531.3 KiB) non-empty blocks including 12 (531.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.318025","level":"info","event":"25/05/28 12:58:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 39 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.438862","level":"info","event":"25/05/28 12:58:18 INFO CodeGenerator: Code generated in 95.364997 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.443186","level":"info","event":"25/05/28 12:58:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.443427","level":"info","event":"25/05/28 12:58:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.444411","level":"info","event":"25/05/28 12:58:18 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.444624","level":"info","event":"25/05/28 12:58:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.444799","level":"info","event":"25/05/28 12:58:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.445093","level":"info","event":"25/05/28 12:58:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.467570","level":"info","event":"25/05/28 12:58:18 INFO CodecConfig: Compression: SNAPPY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.473797","level":"info","event":"25/05/28 12:58:18 INFO CodecConfig: Compression: SNAPPY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.557881","level":"info","event":"25/05/28 12:58:18 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.676326","level":"info","event":"25/05/28 12:58:18 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.676669","level":"info","event":"{","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.676851","level":"info","event":"\"type\" : \"struct\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.676978","level":"info","event":"\"fields\" : [ {","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.677095","level":"info","event":"\"name\" : \"product_name\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.677247","level":"info","event":"\"type\" : \"string\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.677364","level":"info","event":"\"nullable\" : true,","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.677508","level":"info","event":"\"metadata\" : { }","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.677627","level":"info","event":"}, {","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.677738","level":"info","event":"\"name\" : \"Product_Category\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.677847","level":"info","event":"\"type\" : \"string\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678018","level":"info","event":"\"nullable\" : true,","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678199","level":"info","event":"\"metadata\" : { }","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678326","level":"info","event":"}, {","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678442","level":"info","event":"\"name\" : \"Product_Brand\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678556","level":"info","event":"\"type\" : \"string\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678672","level":"info","event":"\"nullable\" : true,","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678784","level":"info","event":"\"metadata\" : { }","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.678903","level":"info","event":"}, {","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679012","level":"info","event":"\"name\" : \"Product_Type\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679141","level":"info","event":"\"type\" : \"string\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679264","level":"info","event":"\"nullable\" : true,","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679408","level":"info","event":"\"metadata\" : { }","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679537","level":"info","event":"} ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679661","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679780","level":"info","event":"and corresponding Parquet message type:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.679898","level":"info","event":"message spark_schema {","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.680012","level":"info","event":"optional binary product_name (STRING);","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.680144","level":"info","event":"optional binary Product_Category (STRING);","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.680267","level":"info","event":"optional binary Product_Brand (STRING);","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.680380","level":"info","event":"optional binary Product_Type (STRING);","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.680489","level":"info","event":"}","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.680599","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:18.680705","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:58:19.778394","level":"info","event":"25/05/28 12:58:19 INFO CodecPool: Got brand-new compressor [.snappy]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:06.919863","level":"info","event":"25/05/28 12:59:06 INFO WriteOperationHelper: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: Retried 0: org.apache.hadoop.fs.s3a.AWSClientIOException: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: minio: Unable to execute HTTP request: minio","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:08.358586","level":"info","event":"25/05/28 12:59:08 INFO WriteOperationHelper: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: Retried 1: org.apache.hadoop.fs.s3a.AWSClientIOException: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: minio: Unable to execute HTTP request: minio","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:09.540293","level":"info","event":"25/05/28 12:59:09 INFO WriteOperationHelper: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: Retried 2: org.apache.hadoop.fs.s3a.AWSClientIOException: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: minio: Unable to execute HTTP request: minio","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:13.606731","level":"info","event":"25/05/28 12:59:13 INFO WriteOperationHelper: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: Retried 3: org.apache.hadoop.fs.s3a.AWSClientIOException: Writing Object on silver/data/products/static/_temporary/0/_temporary/attempt_202505281258174894407011877319903_0004_m_000000_25/part-00000-4471d1e8-dd19-4820-bc85-f0552f41369a-c000.snappy.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: minio: Unable to execute HTTP request: minio","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.175615","level":"info","event":"25/05/28 12:59:20 INFO FileOutputCommitter: Saved output of task 'attempt_202505281258174894407011877319903_0004_m_000000_25' to s3a://etl-dag/silver/data/products/static/_temporary/0/task_202505281258174894407011877319903_0004_m_000000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.177111","level":"info","event":"25/05/28 12:59:20 INFO SparkHadoopMapRedUtil: attempt_202505281258174894407011877319903_0004_m_000000_25: Committed. Elapsed time: 784 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.188344","level":"info","event":"25/05/28 12:59:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 25). 5065 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.217728","level":"info","event":"25/05/28 12:59:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 25) in 62075 ms on localhost (executor driver) (1/1)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.217955","level":"info","event":"25/05/28 12:59:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.232806","level":"info","event":"25/05/28 12:59:20 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 62.152 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.233338","level":"info","event":"25/05/28 12:59:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.233514","level":"info","event":"25/05/28 12:59:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.235298","level":"info","event":"25/05/28 12:59:20 INFO DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 62.565673 s","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.254212","level":"info","event":"25/05/28 12:59:20 INFO FileFormatWriter: Start to commit write Job f441cff2-8bfa-40bb-9a7d-c5ca48798f2a.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.961564","level":"info","event":"25/05/28 12:59:20 INFO FileFormatWriter: Write Job f441cff2-8bfa-40bb-9a7d-c5ca48798f2a committed. Elapsed time: 705 ms.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:20.970792","level":"info","event":"25/05/28 12:59:20 INFO FileFormatWriter: Finished processing stats for write job f441cff2-8bfa-40bb-9a7d-c5ca48798f2a.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.281751","level":"info","event":"25/05/28 12:59:21 INFO FileSourceStrategy: Pushed Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.282123","level":"info","event":"25/05/28 12:59:21 INFO FileSourceStrategy: Post-Scan Filters:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.762723","level":"info","event":"25/05/28 12:59:21 INFO CodeGenerator: Code generated in 168.180981 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.766882","level":"info","event":"25/05/28 12:59:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 211.4 KiB, free 1048.0 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.807576","level":"info","event":"25/05/28 12:59:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 1047.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.813762","level":"info","event":"25/05/28 12:59:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37699 (size: 35.3 KiB, free: 1048.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.814731","level":"info","event":"25/05/28 12:59:21 INFO SparkContext: Created broadcast 7 from parquet at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.818557","level":"info","event":"25/05/28 12:59:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7426254 bytes, open cost is considered as scanning 4194304 bytes.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.894883","level":"info","event":"25/05/28 12:59:21 INFO DAGScheduler: Registering RDD 20 (parquet at NativeMethodAccessorImpl.java:0) as input to shuffle 1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.897596","level":"info","event":"25/05/28 12:59:21 INFO DAGScheduler: Got map stage job 4 (parquet at NativeMethodAccessorImpl.java:0) with 12 output partitions","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.897891","level":"info","event":"25/05/28 12:59:21 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.898066","level":"info","event":"25/05/28 12:59:21 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.898219","level":"info","event":"25/05/28 12:59:21 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.898363","level":"info","event":"25/05/28 12:59:21 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.903110","level":"info","event":"25/05/28 12:59:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 56.3 KiB, free 1047.9 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.943324","level":"info","event":"25/05/28 12:59:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 24.2 KiB, free 1047.8 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.946139","level":"info","event":"25/05/28 12:59:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:37699 (size: 24.2 KiB, free: 1048.6 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.947161","level":"info","event":"25/05/28 12:59:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.948243","level":"info","event":"25/05/28 12:59:21 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.948502","level":"info","event":"25/05/28 12:59:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 12 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.954416","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 26) (localhost, executor driver, partition 0, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.954911","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 27) (localhost, executor driver, partition 1, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.955661","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 28) (localhost, executor driver, partition 2, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.955889","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 29) (localhost, executor driver, partition 3, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.963085","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 30) (localhost, executor driver, partition 4, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.963308","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 31) (localhost, executor driver, partition 5, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.963448","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 32) (localhost, executor driver, partition 6, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.963578","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 33) (localhost, executor driver, partition 7, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.963724","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 34) (localhost, executor driver, partition 8, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.963852","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 35) (localhost, executor driver, partition 9, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.963967","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 36) (localhost, executor driver, partition 10, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.964081","level":"info","event":"25/05/28 12:59:21 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 37) (localhost, executor driver, partition 11, PROCESS_LOCAL, 8208 bytes)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.964204","level":"info","event":"25/05/28 12:59:21 INFO Executor: Running task 0.0 in stage 5.0 (TID 26)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.965189","level":"info","event":"25/05/28 12:59:21 INFO Executor: Running task 1.0 in stage 5.0 (TID 27)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.967661","level":"info","event":"25/05/28 12:59:21 INFO Executor: Running task 2.0 in stage 5.0 (TID 28)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.972554","level":"info","event":"25/05/28 12:59:21 INFO Executor: Running task 3.0 in stage 5.0 (TID 29)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.973047","level":"info","event":"25/05/28 12:59:21 INFO Executor: Running task 4.0 in stage 5.0 (TID 30)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.973664","level":"info","event":"25/05/28 12:59:21 INFO Executor: Running task 5.0 in stage 5.0 (TID 31)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:21.983343","level":"info","event":"25/05/28 12:59:21 INFO Executor: Running task 6.0 in stage 5.0 (TID 32)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.020494","level":"info","event":"25/05/28 12:59:22 INFO Executor: Running task 7.0 in stage 5.0 (TID 33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.022383","level":"info","event":"25/05/28 12:59:22 INFO Executor: Running task 9.0 in stage 5.0 (TID 35)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.025603","level":"info","event":"25/05/28 12:59:22 INFO Executor: Running task 8.0 in stage 5.0 (TID 34)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.042851","level":"info","event":"25/05/28 12:59:22 INFO Executor: Running task 11.0 in stage 5.0 (TID 37)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.043732","level":"info","event":"25/05/28 12:59:22 INFO Executor: Running task 10.0 in stage 5.0 (TID 36)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.132303","level":"info","event":"25/05/28 12:59:22 INFO CodeGenerator: Code generated in 76.236127 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.172960","level":"info","event":"25/05/28 12:59:22 INFO CodeGenerator: Code generated in 28.734881 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.252188","level":"info","event":"25/05/28 12:59:22 INFO CodeGenerator: Code generated in 23.287416 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.310136","level":"info","event":"25/05/28 12:59:22 INFO CodeGenerator: Code generated in 14.616259 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.362628","level":"info","event":"25/05/28 12:59:22 INFO CodeGenerator: Code generated in 44.188681 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.391476","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 51983778-59410032, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.396113","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 29705016-37131270, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.397327","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 44557524-51983778, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.399593","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 59410032-66836286, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.399897","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 22278762-29705016, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.400358","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 37131270-44557524, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.404895","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 0-7426254, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.406584","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 66836286-74262540, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.409152","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 81688794-84920745, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.409453","level":"info","event":"25/05/28 12:59:22 INFO CodeGenerator: Code generated in 14.649159 ms","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.409948","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 74262540-81688794, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.412051","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 14852508-22278762, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:22.415033","level":"info","event":"25/05/28 12:59:22 INFO FileScanRDD: Reading File path: s3a://etl-dag/bronze/data/retail_data.csv, range: 7426254-14852508, partition values: [empty row]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T12:59:24.035938","level":"info","event":"25/05/28 12:59:24 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:37699 in memory (size: 93.6 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:24.460269","level":"info","event":"25/05/28 13:00:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:37699 in memory (size: 35.3 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:24.732689","level":"info","event":"25/05/28 13:00:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:37699 in memory (size: 19.1 KiB, free: 1048.7 MiB)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:25.558532","level":"info","event":"25/05/28 13:00:25 INFO S3AInputStream: Got exception while trying to read from stream s3a://etl-dag/bronze/data/retail_data.csv, client: org.apache.hadoop.fs.s3a.S3AFileSystem$InputStreamCallbacksImpl@ed85568 object: S3Object [key=bronze/data/retail_data.csv,bucket=etl-dag], trying to recover: com.amazonaws.thirdparty.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 47,789,475; received: 7,005,826)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:25.662106","level":"info","event":"25/05/28 13:00:25 INFO S3AInputStream: Got exception while trying to read from stream s3a://etl-dag/bronze/data/retail_data.csv, client: org.apache.hadoop.fs.s3a.S3AFileSystem$InputStreamCallbacksImpl@40dbd65b object: S3Object [key=bronze/data/retail_data.csv,bucket=etl-dag], trying to recover: com.amazonaws.thirdparty.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 84,920,745; received: 6,207,248)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:25.665831","level":"info","event":"25/05/28 13:00:25 INFO S3AInputStream: Got exception while trying to read from stream s3a://etl-dag/bronze/data/retail_data.csv, client: org.apache.hadoop.fs.s3a.S3AFileSystem$InputStreamCallbacksImpl@4c46554f object: S3Object [key=bronze/data/retail_data.csv,bucket=etl-dag], trying to recover: com.amazonaws.thirdparty.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 18,084,459; received: 7,258,139)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:25.718677","level":"info","event":"25/05/28 13:00:25 INFO S3AInputStream: Got exception while trying to read from stream s3a://etl-dag/bronze/data/retail_data.csv, client: org.apache.hadoop.fs.s3a.S3AFileSystem$InputStreamCallbacksImpl@1cf93e7 object: S3Object [key=bronze/data/retail_data.csv,bucket=etl-dag], trying to recover: com.amazonaws.thirdparty.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 32,936,967; received: 4,667,407)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:26.012804","level":"info","event":"25/05/28 13:00:26 INFO S3AInputStream: Got exception while trying to read from stream s3a://etl-dag/bronze/data/retail_data.csv, client: org.apache.hadoop.fs.s3a.S3AFileSystem$InputStreamCallbacksImpl@4813c7a1 object: S3Object [key=bronze/data/retail_data.csv,bucket=etl-dag], trying to recover: com.amazonaws.thirdparty.apache.http.ConnectionClosedException: Premature end of Content-Length delimited message body (expected: 55,215,729; received: 7,145,512)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:26.056473","level":"info","event":"25/05/28 13:00:26 INFO Executor: Finished task 11.0 in stage 5.0 (TID 37). 2791 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:26.072613","level":"info","event":"25/05/28 13:00:26 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 37) in 64107 ms on localhost (executor driver) (1/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:26.937507","level":"info","event":"25/05/28 13:00:26 INFO Executor: Finished task 3.0 in stage 5.0 (TID 29). 2748 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:26.943097","level":"info","event":"25/05/28 13:00:26 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 29) in 64986 ms on localhost (executor driver) (2/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.069587","level":"info","event":"25/05/28 13:00:27 INFO Executor: Finished task 10.0 in stage 5.0 (TID 36). 2791 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.071384","level":"info","event":"25/05/28 13:00:27 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 36) in 65110 ms on localhost (executor driver) (3/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.310842","level":"info","event":"25/05/28 13:00:27 INFO Executor: Finished task 8.0 in stage 5.0 (TID 34). 2748 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.312580","level":"info","event":"25/05/28 13:00:27 INFO Executor: Finished task 1.0 in stage 5.0 (TID 27). 2748 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.317372","level":"info","event":"25/05/28 13:00:27 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 34) in 65354 ms on localhost (executor driver) (4/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.320736","level":"info","event":"25/05/28 13:00:27 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 27) in 65364 ms on localhost (executor driver) (5/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.345522","level":"info","event":"25/05/28 13:00:27 INFO Executor: Finished task 2.0 in stage 5.0 (TID 28). 2748 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.348633","level":"info","event":"25/05/28 13:00:27 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 28) in 65394 ms on localhost (executor driver) (6/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.495845","level":"info","event":"25/05/28 13:00:27 INFO Executor: Finished task 6.0 in stage 5.0 (TID 32). 2748 bytes result sent to driver","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:00:27.502477","level":"info","event":"25/05/28 13:00:27 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 32) in 65538 ms on localhost (executor driver) (7/12)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T13:02:36.446923","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master local[*] --conf spark.master=local[*] --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioLocalAccessKey --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.hadoop.fs.s3a.attempts.maximum=3 --conf spark.hadoop.fs.s3a.connection.establish.timeout=5000 --conf spark.hadoop.fs.s3a.connection.timeout=200000 --conf spark.hadoop.fs.s3a.buffer.dir=/tmp --conf spark.hadoop.fs.s3a.fast.upload=true --conf spark.hadoop.fs.s3a.fast.upload.buffer=disk --conf spark.hadoop.fs.s3a.multipart.size=104857600 --conf spark.hadoop.fs.s3a.multipart.threshold=2147483647 --driver-memory 2g --name split-product-local --verbose --deploy-mode client /opt/airflow/dags/spark_jobs/split_product.py. Error code is: -9.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}]}]}
