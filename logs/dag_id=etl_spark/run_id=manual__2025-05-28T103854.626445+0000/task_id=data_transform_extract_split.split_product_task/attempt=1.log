{"timestamp":"2025-05-28T10:41:06.395647","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-28T10:41:06.396429","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/ETL_dags.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-28T10:41:07.548023","level":"info","event":"Secrets backends loaded for worker","count":1,"backend_classes":["EnvironmentVariablesBackend"],"logger":"supervisor"}
{"timestamp":"2025-05-28T10:41:07.581362","level":"warning","event":"Skipping masking for a secret as it's too short (<5 chars)","logger":"airflow.sdk.execution_time.secrets_masker"}
{"timestamp":"2025-05-28T10:41:07.581957","level":"info","event":"Connection Retrieved 'spark_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-05-28T10:41:07.583259","level":"info","event":"Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.master=local[*] --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioLocalAccessKey --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --name arrow-spark --verbose /opt/airflow/dags/spark_jobs/split_product.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.126771","level":"info","event":"Using properties file: /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.315700","level":"info","event":"Adding default property: spark.executor.extraClassPath=/opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.316015","level":"info","event":"Adding default property: spark.sql.adaptive.coalescePartitions.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.317561","level":"info","event":"Adding default property: spark.sql.adaptive.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.317827","level":"info","event":"Adding default property: spark.driver.extraClassPath=/opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.353332","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.353611","level":"info","event":"master                  spark://spark:7077","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.353997","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.354790","level":"info","event":"deployMode              null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.356480","level":"info","event":"executorMemory          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.357522","level":"info","event":"executorCores           null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.357702","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.357812","level":"info","event":"propertiesFile          /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358092","level":"info","event":"driverMemory            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358195","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358308","level":"info","event":"driverExtraClassPath    /opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358407","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358503","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358595","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358687","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358778","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.358917","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.359017","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.359107","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.359323","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.361225","level":"info","event":"primaryResource         file:/opt/airflow/dags/spark_jobs/split_product.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.361687","level":"info","event":"name                    arrow-spark","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.361975","level":"info","event":"childArgs               []","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.362580","level":"info","event":"jars                    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.362933","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.363327","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.363510","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.363638","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.363795","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.363919","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.364041","level":"info","event":"--conf and those from the properties file /opt/spark/conf/spark-defaults.conf:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.364164","level":"info","event":"(spark.driver.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.364372","level":"info","event":"(spark.executor.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.364503","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.364617","level":"info","event":"(spark.hadoop.fs.s3a.aws.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.364768","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.364914","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.365040","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.365155","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.365262","level":"info","event":"(spark.master,local[*])","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.365430","level":"info","event":"(spark.sql.adaptive.coalescePartitions.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.365543","level":"info","event":"(spark.sql.adaptive.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.365659","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.365772","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.831879","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.832146","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.832704","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.832851","level":"info","event":"file:/opt/airflow/dags/spark_jobs/split_product.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.832963","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.837599","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.838249","level":"info","event":"(spark.app.name,arrow-spark)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.838397","level":"info","event":"(spark.app.submitTime,1748428870804)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.838488","level":"info","event":"(spark.driver.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.838576","level":"info","event":"(spark.executor.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.838841","level":"info","event":"(spark.hadoop.fs.s3a.access.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839007","level":"info","event":"(spark.hadoop.fs.s3a.aws.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839128","level":"info","event":"(spark.hadoop.fs.s3a.endpoint,http://minio:9000)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839235","level":"info","event":"(spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839341","level":"info","event":"(spark.hadoop.fs.s3a.path.style.access,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839442","level":"info","event":"(spark.hadoop.fs.s3a.secret.key,*********(redacted))","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839547","level":"info","event":"(spark.master,spark://spark:7077)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839681","level":"info","event":"(spark.sql.adaptive.coalescePartitions.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839799","level":"info","event":"(spark.sql.adaptive.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.839903","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.840057","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.840199","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.840306","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.840411","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:10.840516","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.692016","level":"info","event":"25/05/28 10:41:12 INFO SparkContext: Running Spark version 3.5.1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.697024","level":"info","event":"25/05/28 10:41:12 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.697480","level":"info","event":"25/05/28 10:41:12 INFO SparkContext: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.787263","level":"info","event":"25/05/28 10:41:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.907687","level":"info","event":"25/05/28 10:41:12 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.908124","level":"info","event":"25/05/28 10:41:12 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.908596","level":"info","event":"25/05/28 10:41:12 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.909054","level":"info","event":"25/05/28 10:41:12 INFO SparkContext: Submitted application: Split Product Data","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.937887","level":"info","event":"25/05/28 10:41:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.949816","level":"info","event":"25/05/28 10:41:12 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:12.951244","level":"info","event":"25/05/28 10:41:12 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.034003","level":"info","event":"25/05/28 10:41:13 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.034763","level":"info","event":"25/05/28 10:41:13 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.035872","level":"info","event":"25/05/28 10:41:13 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.036880","level":"info","event":"25/05/28 10:41:13 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.037676","level":"info","event":"25/05/28 10:41:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.405161","level":"info","event":"25/05/28 10:41:13 INFO Utils: Successfully started service 'sparkDriver' on port 35391.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.458997","level":"info","event":"25/05/28 10:41:13 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.553516","level":"info","event":"25/05/28 10:41:13 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.586389","level":"info","event":"25/05/28 10:41:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.587824","level":"info","event":"25/05/28 10:41:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.600734","level":"info","event":"25/05/28 10:41:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.706486","level":"info","event":"25/05/28 10:41:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-63973d33-2cd4-44bd-ae7e-cd56405647d8","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.725305","level":"info","event":"25/05/28 10:41:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.750671","level":"info","event":"25/05/28 10:41:13 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:13.931868","level":"info","event":"25/05/28 10:41:13 INFO JettyUtils: Start Jetty 127.0.0.1:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.018781","level":"info","event":"25/05/28 10:41:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.032240","level":"info","event":"25/05/28 10:41:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.326168","level":"info","event":"25/05/28 10:41:14 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.392029","level":"info","event":"25/05/28 10:41:14 INFO TransportClientFactory: Successfully created connection to spark/172.19.0.2:7077 after 37 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.581544","level":"info","event":"25/05/28 10:41:14 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250528104114-0006","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.613600","level":"info","event":"25/05/28 10:41:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37625.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.614346","level":"info","event":"25/05/28 10:41:14 INFO NettyBlockTransferService: Server created on localhost:37625","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.619076","level":"info","event":"25/05/28 10:41:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.633516","level":"info","event":"25/05/28 10:41:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 37625, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.644552","level":"info","event":"25/05/28 10:41:14 INFO BlockManagerMasterEndpoint: Registering block manager localhost:37625 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 37625, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.649615","level":"info","event":"25/05/28 10:41:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 37625, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:14.652239","level":"info","event":"25/05/28 10:41:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 37625, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:15.460064","level":"info","event":"25/05/28 10:41:15 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:16.111311","level":"info","event":"25/05/28 10:41:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:16.135692","level":"info","event":"25/05/28 10:41:16 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:19.008201","level":"info","event":"25/05/28 10:41:19 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:19.051095","level":"info","event":"25/05/28 10:41:19 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:19.051843","level":"info","event":"25/05/28 10:41:19 INFO MetricsSystemImpl: s3a-file-system metrics system started","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.265616","level":"info","event":"25/05/28 10:41:20 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.265931","level":"info","event":"com.amazonaws.SdkClientException: Failed to connect to service endpoint:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.266091","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.266219","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.266361","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.266524","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.266655","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsEndpoint(InstanceMetadataServiceCredentialsFetcher.java:60)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.266787","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsResponse(InstanceMetadataServiceCredentialsFetcher.java:48)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.266954","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.fetchCredentials(BaseCredentialsFetcher.java:124)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267086","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.getCredentials(BaseCredentialsFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267200","level":"info","event":"at com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267310","level":"info","event":"at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.getCredentials(EC2ContainerCredentialsProviderWrapper.java:75)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267521","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267640","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267747","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267847","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.267946","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268076","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268182","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268285","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268411","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268521","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268658","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268777","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.268915","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.269059","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.269205","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.269359","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.269492","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.269616","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.269766","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.269928","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.270122","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.270277","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.270387","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$isDirectory$35(S3AFileSystem.java:4724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.270492","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.270595","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.270804","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.270942","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273113","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273269","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273367","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273496","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273587","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273785","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273866","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.273940","level":"info","event":"at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274068","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274147","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274220","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274294","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274366","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274437","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274541","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274731","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274818","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.274959","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275114","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275209","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275287","level":"info","event":"Caused by: java.net.ConnectException: Connection refused","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275378","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnect(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275510","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275705","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275832","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.275955","level":"info","event":"at java.base/java.net.Socket.connect(Socket.java:633)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276072","level":"info","event":"at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276218","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276337","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276446","level":"info","event":"at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:281)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276672","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276806","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276895","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1324)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.276976","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1302)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.277055","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1143)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.277132","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1072)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.277205","level":"info","event":"at com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.277281","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.277490","level":"info","event":"... 56 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.291481","level":"info","event":"25/05/28 10:41:20 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://etl-dag/bronze/data/retail_data.csv.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.291659","level":"info","event":"java.nio.file.AccessDeniedException: s3a://etl-dag/bronze/data/retail_data.csv: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2ac1a407: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4713f8ba: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.291785","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.292522","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.292665","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3861)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.292775","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.292881","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$isDirectory$35(S3AFileSystem.java:4724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293044","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293150","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293252","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293350","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293492","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293605","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293735","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.293939","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.294410","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.294557","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.295118","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.295306","level":"info","event":"at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.295426","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.295534","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.296567","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.296922","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.297377","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.297664","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.297837","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.298050","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.298215","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.298416","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.298530","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.298630","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.298812","level":"info","event":"Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2ac1a407: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4713f8ba: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.299062","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.302196","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.302409","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.302591","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.302724","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.302841","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.302959","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303125","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303255","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303391","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303492","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303632","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303765","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303864","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.303976","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.304083","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.304239","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.304378","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.304493","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.304600","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.304750","level":"info","event":"... 26 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.304882","level":"info","event":"Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2ac1a407: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4713f8ba: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.305009","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.305125","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.305291","level":"info","event":"... 45 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.356638","level":"info","event":"25/05/28 10:41:20 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.356853","level":"info","event":"com.amazonaws.SdkClientException: Failed to connect to service endpoint:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.356978","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.357082","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.357202","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.357397","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.357562","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsEndpoint(InstanceMetadataServiceCredentialsFetcher.java:60)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.357724","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsResponse(InstanceMetadataServiceCredentialsFetcher.java:48)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.359153","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.fetchCredentials(BaseCredentialsFetcher.java:124)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.359305","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.getCredentials(BaseCredentialsFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.359522","level":"info","event":"at com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.359689","level":"info","event":"at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.getCredentials(EC2ContainerCredentialsProviderWrapper.java:75)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.359811","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.359922","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.360136","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.360259","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.361343","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.361538","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.361661","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.361814","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.366617","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.366954","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.367103","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.367277","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.367407","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.367528","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.367652","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.367822","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.367946","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.368061","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.368179","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.368389","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.368520","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.368646","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.368764","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.368989","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.369123","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.369549","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.370263","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.370459","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.370597","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.370733","level":"info","event":"at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.370908","level":"info","event":"at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.371046","level":"info","event":"at scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.371182","level":"info","event":"at scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.371314","level":"info","event":"at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.371503","level":"info","event":"at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.371630","level":"info","event":"at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.371753","level":"info","event":"at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.371867","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.372016","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.372116","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.372208","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.372299","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.372391","level":"info","event":"at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.372805","level":"info","event":"Caused by: java.net.ConnectException: Connection refused","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.373098","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnect(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.373341","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.373571","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.373793","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.374487","level":"info","event":"at java.base/java.net.Socket.connect(Socket.java:633)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.374762","level":"info","event":"at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.374908","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.375016","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.375430","level":"info","event":"at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:281)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.375649","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.375777","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.375915","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1324)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.376061","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1302)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.376180","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1143)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.376627","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1072)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.376820","level":"info","event":"at com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.376985","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.377127","level":"info","event":"... 52 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.660330","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.660623","level":"info","event":"File \"/opt/airflow/dags/spark_jobs/split_product.py\", line 7, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.667791","level":"info","event":"df = spark.read.option(\"header\", True).csv(","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.668027","level":"info","event":"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.668172","level":"info","event":"File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 740, in csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.674227","level":"info","event":"File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.675280","level":"info","event":"File \"/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.676311","level":"info","event":"File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684030","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o36.csv.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684257","level":"info","event":": java.nio.file.AccessDeniedException: s3a://etl-dag/bronze/data/retail_data.csv: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2ac1a407: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4713f8ba: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684381","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684478","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684570","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3799)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684661","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684748","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684832","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.684920","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685007","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685093","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685179","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685267","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685353","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685442","level":"info","event":"at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685619","level":"info","event":"at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685722","level":"info","event":"at scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685813","level":"info","event":"at scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685900","level":"info","event":"at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.685987","level":"info","event":"at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686075","level":"info","event":"at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686163","level":"info","event":"at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686254","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686346","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686439","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686579","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686705","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686813","level":"info","event":"at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.686921","level":"info","event":"Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2ac1a407: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4713f8ba: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687040","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687152","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687261","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687368","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687475","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687661","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687786","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.687899","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.688010","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.688127","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.688247","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.688360","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.688491","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.688605","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.688717","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.689931","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.690430","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.690585","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.690713","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.690832","level":"info","event":"... 23 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.690955","level":"info","event":"Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@2ac1a407: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@4713f8ba: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.691080","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.691200","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.691315","level":"info","event":"... 41 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.691443","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.814805","level":"info","event":"25/05/28 10:41:20 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.815486","level":"info","event":"25/05/28 10:41:20 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.840185","level":"info","event":"25/05/28 10:41:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250528104114-0006/0 on worker-20250528102647-172.19.0.8-36983 (172.19.0.8:36983) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.847106","level":"info","event":"25/05/28 10:41:20 INFO SparkUI: Stopped Spark web UI at http://localhost:4041","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.848031","level":"info","event":"25/05/28 10:41:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20250528104114-0006/0 on hostPort 172.19.0.8:36983 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.856087","level":"info","event":"25/05/28 10:41:20 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.860958","level":"info","event":"25/05/28 10:41:20 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.891193","level":"info","event":"25/05/28 10:41:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.914899","level":"info","event":"25/05/28 10:41:20 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.915601","level":"info","event":"25/05/28 10:41:20 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.929103","level":"info","event":"25/05/28 10:41:20 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.933559","level":"info","event":"25/05/28 10:41:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.951355","level":"info","event":"25/05/28 10:41:20 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.951988","level":"info","event":"25/05/28 10:41:20 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.956523","level":"info","event":"25/05/28 10:41:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-952dc704-c720-4ad4-8162-d991f2c467ce","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.964457","level":"info","event":"25/05/28 10:41:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-952dc704-c720-4ad4-8162-d991f2c467ce/pyspark-619f29fb-48b2-427b-a74e-7a49b2ea8cd6","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.969765","level":"info","event":"25/05/28 10:41:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-12ac8729-f633-4d59-80fb-c3444b84e894","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.976254","level":"info","event":"25/05/28 10:41:20 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.976505","level":"info","event":"25/05/28 10:41:20 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:20.976616","level":"info","event":"25/05/28 10:41:20 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:41:21.029991","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master spark://spark:7077 --conf spark.master=local[*] --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioLocalAccessKey --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --name arrow-spark --verbose /opt/airflow/dags/spark_jobs/split_product.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}]}]}
