{"timestamp":"2025-05-28T10:10:07.179745","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-05-28T10:10:07.180488","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/ETL_dags.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-05-28T10:10:08.350597","level":"info","event":"Secrets backends loaded for worker","count":1,"backend_classes":["EnvironmentVariablesBackend"],"logger":"supervisor"}
{"timestamp":"2025-05-28T10:10:08.373088","level":"warning","event":"Skipping masking for a secret as it's too short (<5 chars)","logger":"airflow.sdk.execution_time.secrets_masker"}
{"timestamp":"2025-05-28T10:10:08.373866","level":"info","event":"Connection Retrieved 'spark_default'","logger":"airflow.hooks.base"}
{"timestamp":"2025-05-28T10:10:08.374747","level":"info","event":"Spark-Submit cmd: spark-submit --master spark://spark:7077 --conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain --name arrow-spark --verbose /opt/airflow/dags/spark_jobs/split_customer.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.653915","level":"info","event":"Using properties file: /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.812747","level":"info","event":"Adding default property: spark.executor.extraClassPath=/opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.813118","level":"info","event":"Adding default property: spark.sql.adaptive.coalescePartitions.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.813410","level":"info","event":"Adding default property: spark.sql.adaptive.enabled=true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.813568","level":"info","event":"Adding default property: spark.driver.extraClassPath=/opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.867232","level":"info","event":"Parsed arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.867429","level":"info","event":"master                  spark://spark:7077","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.867563","level":"info","event":"remote                  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.867689","level":"info","event":"deployMode              null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.867812","level":"info","event":"executorMemory          null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.867954","level":"info","event":"executorCores           null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868077","level":"info","event":"totalExecutorCores      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868197","level":"info","event":"propertiesFile          /opt/spark/conf/spark-defaults.conf","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868319","level":"info","event":"driverMemory            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868438","level":"info","event":"driverCores             null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868560","level":"info","event":"driverExtraClassPath    /opt/spark-jars/*","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868681","level":"info","event":"driverExtraLibraryPath  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868798","level":"info","event":"driverExtraJavaOptions  null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.868935","level":"info","event":"supervise               false","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869060","level":"info","event":"queue                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869177","level":"info","event":"numExecutors            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869293","level":"info","event":"files                   null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869409","level":"info","event":"pyFiles                 null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869526","level":"info","event":"archives                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869642","level":"info","event":"mainClass               null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869760","level":"info","event":"primaryResource         file:/opt/airflow/dags/spark_jobs/split_customer.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.869877","level":"info","event":"name                    arrow-spark","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870022","level":"info","event":"childArgs               []","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870142","level":"info","event":"jars                    null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870257","level":"info","event":"packages                null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870373","level":"info","event":"packagesExclusions      null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870491","level":"info","event":"repositories            null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870609","level":"info","event":"verbose                 true","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870726","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.870845","level":"info","event":"Spark properties used, including those specified through","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.871004","level":"info","event":"--conf and those from the properties file /opt/spark/conf/spark-defaults.conf:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.871499","level":"info","event":"(spark.driver.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.871648","level":"info","event":"(spark.executor.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.871763","level":"info","event":"(spark.hadoop.fs.s3a.aws.credentials.provider,com.amazonaws.auth.DefaultAWSCredentialsProviderChain)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.871874","level":"info","event":"(spark.sql.adaptive.coalescePartitions.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.872021","level":"info","event":"(spark.sql.adaptive.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.872152","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:10.872278","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.303574","level":"info","event":"Main class:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.303870","level":"info","event":"org.apache.spark.deploy.PythonRunner","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.304707","level":"info","event":"Arguments:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.304978","level":"info","event":"file:/opt/airflow/dags/spark_jobs/split_customer.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.305196","level":"info","event":"null","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.309558","level":"info","event":"Spark config:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.309826","level":"info","event":"(spark.app.name,arrow-spark)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310024","level":"info","event":"(spark.app.submitTime,1748427011277)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310164","level":"info","event":"(spark.driver.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310281","level":"info","event":"(spark.executor.extraClassPath,/opt/spark-jars/*)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310393","level":"info","event":"(spark.hadoop.fs.s3a.aws.credentials.provider,com.amazonaws.auth.DefaultAWSCredentialsProviderChain)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310516","level":"info","event":"(spark.master,spark://spark:7077)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310641","level":"info","event":"(spark.sql.adaptive.coalescePartitions.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310762","level":"info","event":"(spark.sql.adaptive.enabled,true)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.310876","level":"info","event":"(spark.submit.deployMode,client)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.311039","level":"info","event":"(spark.submit.pyFiles,)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.311188","level":"info","event":"Classpath elements:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.311312","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.311420","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:11.311524","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.051551","level":"info","event":"25/05/28 10:10:13 INFO SparkContext: Running Spark version 3.5.1","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.054479","level":"info","event":"25/05/28 10:10:13 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.054930","level":"info","event":"25/05/28 10:10:13 INFO SparkContext: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.152526","level":"info","event":"25/05/28 10:10:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.294621","level":"info","event":"25/05/28 10:10:13 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.295298","level":"info","event":"25/05/28 10:10:13 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.295933","level":"info","event":"25/05/28 10:10:13 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.296927","level":"info","event":"25/05/28 10:10:13 INFO SparkContext: Submitted application: split_customer","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.335047","level":"info","event":"25/05/28 10:10:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.349477","level":"info","event":"25/05/28 10:10:13 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.350756","level":"info","event":"25/05/28 10:10:13 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.441367","level":"info","event":"25/05/28 10:10:13 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.442471","level":"info","event":"25/05/28 10:10:13 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.443585","level":"info","event":"25/05/28 10:10:13 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.444507","level":"info","event":"25/05/28 10:10:13 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.445336","level":"info","event":"25/05/28 10:10:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:13.915615","level":"info","event":"25/05/28 10:10:13 INFO Utils: Successfully started service 'sparkDriver' on port 44619.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.053474","level":"info","event":"25/05/28 10:10:14 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.128332","level":"info","event":"25/05/28 10:10:14 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.154450","level":"info","event":"25/05/28 10:10:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.155876","level":"info","event":"25/05/28 10:10:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.164732","level":"info","event":"25/05/28 10:10:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.299343","level":"info","event":"25/05/28 10:10:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb02a8e7-3e8e-41ad-ba97-b1a58741562c","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.318617","level":"info","event":"25/05/28 10:10:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.342941","level":"info","event":"25/05/28 10:10:14 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.565975","level":"info","event":"25/05/28 10:10:14 INFO JettyUtils: Start Jetty 127.0.0.1:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.665501","level":"info","event":"25/05/28 10:10:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.857145","level":"info","event":"25/05/28 10:10:14 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark:7077...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:14.925591","level":"info","event":"25/05/28 10:10:14 INFO TransportClientFactory: Successfully created connection to spark/172.19.0.3:7077 after 39 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.267032","level":"info","event":"25/05/28 10:10:15 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250528101015-0000","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.277476","level":"info","event":"25/05/28 10:10:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40805.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.277671","level":"info","event":"25/05/28 10:10:15 INFO NettyBlockTransferService: Server created on localhost:40805","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.280629","level":"info","event":"25/05/28 10:10:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.288344","level":"info","event":"25/05/28 10:10:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 40805, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.292136","level":"info","event":"25/05/28 10:10:15 INFO BlockManagerMasterEndpoint: Registering block manager localhost:40805 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 40805, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.295351","level":"info","event":"25/05/28 10:10:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 40805, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.296197","level":"info","event":"25/05/28 10:10:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250528101015-0000/0 on worker-20250528095539-172.19.0.8-44857 (172.19.0.8:44857) with 12 core(s)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.297232","level":"info","event":"25/05/28 10:10:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 40805, None)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.300280","level":"info","event":"25/05/28 10:10:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20250528101015-0000/0 on hostPort 172.19.0.8:44857 with 12 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:15.710159","level":"info","event":"25/05/28 10:10:15 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:16.223744","level":"info","event":"25/05/28 10:10:16 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250528101015-0000/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:16.403910","level":"info","event":"25/05/28 10:10:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:16.423934","level":"info","event":"25/05/28 10:10:16 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:18.622052","level":"info","event":"25/05/28 10:10:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:18.645706","level":"info","event":"25/05/28 10:10:18 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:18.646413","level":"info","event":"25/05/28 10:10:18 INFO MetricsSystemImpl: s3a-file-system metrics system started","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.270351","level":"info","event":"25/05/28 10:10:20 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.270676","level":"info","event":"com.amazonaws.SdkClientException: Failed to connect to service endpoint:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.270819","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.270942","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271088","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271204","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271316","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsEndpoint(InstanceMetadataServiceCredentialsFetcher.java:60)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271427","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsResponse(InstanceMetadataServiceCredentialsFetcher.java:48)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271536","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.fetchCredentials(BaseCredentialsFetcher.java:124)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271645","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.getCredentials(BaseCredentialsFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271758","level":"info","event":"at com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271872","level":"info","event":"at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.getCredentials(EC2ContainerCredentialsProviderWrapper.java:75)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.271975","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272111","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272221","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272328","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272432","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272539","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272650","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272769","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272888","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.272978","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273108","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273208","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273300","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273400","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273501","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273594","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273719","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273818","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.273906","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274018","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274131","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274221","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274308","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274396","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274483","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274577","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$isDirectory$35(S3AFileSystem.java:4724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274669","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274814","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.274980","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.280615","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.280793","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.280902","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281019","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281114","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281190","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281264","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281349","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281438","level":"info","event":"at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281533","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281628","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281730","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.281821","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282043","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282156","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282252","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282340","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282428","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282533","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282629","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282835","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.282938","level":"info","event":"Caused by: java.net.ConnectException: Connection refused","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.283305","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnect(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.283499","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.283692","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.283904","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.284049","level":"info","event":"at java.base/java.net.Socket.connect(Socket.java:633)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.284208","level":"info","event":"at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.284634","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.284775","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.284891","level":"info","event":"at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:281)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.285030","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.285150","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.285261","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1324)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.285401","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1302)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.285529","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1143)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.285781","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1072)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.285932","level":"info","event":"at com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.286104","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.286230","level":"info","event":"... 59 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.286441","level":"info","event":"25/05/28 10:10:20 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://etl-dag/data/retail_data.csv.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.286582","level":"info","event":"java.nio.file.AccessDeniedException: s3a://etl-dag/data/retail_data.csv: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@72b1251a: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@2a90d44: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.286731","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.286838","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.286946","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3861)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.287082","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.287196","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$isDirectory$35(S3AFileSystem.java:4724)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.287312","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.287425","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.287541","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.287657","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.287772","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4722)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288067","level":"info","event":"at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288212","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288328","level":"info","event":"at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288437","level":"info","event":"at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288543","level":"info","event":"at scala.Option.getOrElse(Option.scala:189)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288645","level":"info","event":"at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288753","level":"info","event":"at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288866","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.288982","level":"info","event":"at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289123","level":"info","event":"at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289217","level":"info","event":"at java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289309","level":"info","event":"at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289401","level":"info","event":"at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289533","level":"info","event":"at py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289658","level":"info","event":"at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289776","level":"info","event":"at py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.289895","level":"info","event":"at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290015","level":"info","event":"at py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290163","level":"info","event":"at java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290286","level":"info","event":"Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@72b1251a: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@2a90d44: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290403","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290520","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290679","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290805","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.290926","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291043","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291179","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291297","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291410","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291516","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291622","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291729","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291830","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.291939","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292060","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292203","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5397)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292311","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:971)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292415","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2595)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292519","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292623","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292728","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292831","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2586)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.292929","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3832)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.293026","level":"info","event":"... 26 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.293157","level":"info","event":"Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@72b1251a: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@2a90d44: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.293276","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.293381","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.293487","level":"info","event":"... 48 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.368357","level":"info","event":"25/05/28 10:10:20 WARN InstanceMetadataServiceResourceFetcher: Fail to retrieve token","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.368601","level":"info","event":"com.amazonaws.SdkClientException: Failed to connect to service endpoint:","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.368782","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:100)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.368905","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.getToken(InstanceMetadataServiceResourceFetcher.java:91)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369018","level":"info","event":"at com.amazonaws.internal.InstanceMetadataServiceResourceFetcher.readResource(InstanceMetadataServiceResourceFetcher.java:69)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369151","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.readResource(EC2ResourceFetcher.java:66)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369260","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsEndpoint(InstanceMetadataServiceCredentialsFetcher.java:60)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369383","level":"info","event":"at com.amazonaws.auth.InstanceMetadataServiceCredentialsFetcher.getCredentialsResponse(InstanceMetadataServiceCredentialsFetcher.java:48)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369523","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.fetchCredentials(BaseCredentialsFetcher.java:124)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369644","level":"info","event":"at com.amazonaws.auth.BaseCredentialsFetcher.getCredentials(BaseCredentialsFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369764","level":"info","event":"at com.amazonaws.auth.InstanceProfileCredentialsProvider.getCredentials(InstanceProfileCredentialsProvider.java:166)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.369885","level":"info","event":"at com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper.getCredentials(EC2ContainerCredentialsProviderWrapper.java:75)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370013","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370205","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370356","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370478","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370602","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370733","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370845","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.370973","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.371105","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.371226","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.371332","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.371448","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.371553","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.371880","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372013","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372139","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372240","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372341","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372460","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372561","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372662","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372765","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372862","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.372957","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373044","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373158","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373367","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373569","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373676","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373770","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373858","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.373946","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.375817","level":"info","event":"at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.376044","level":"info","event":"at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.376368","level":"info","event":"at scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.376706","level":"info","event":"at scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.376922","level":"info","event":"at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.377039","level":"info","event":"at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.377425","level":"info","event":"at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.377749","level":"info","event":"at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.377941","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.378206","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.378388","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.378847","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.378976","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.379125","level":"info","event":"at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.379292","level":"info","event":"Caused by: java.net.ConnectException: Connection refused","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.379485","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnect(Native Method)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.379599","level":"info","event":"at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.379729","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.379839","level":"info","event":"at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.379939","level":"info","event":"at java.base/java.net.Socket.connect(Socket.java:633)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.380275","level":"info","event":"at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:178)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.380490","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.380630","level":"info","event":"at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.380876","level":"info","event":"at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:281)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.380994","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.381155","level":"info","event":"at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.381283","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1324)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.381391","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1302)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.382513","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1143)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.382708","level":"info","event":"at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1072)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.382894","level":"info","event":"at com.amazonaws.internal.ConnectionUtils.connectToEndpoint(ConnectionUtils.java:95)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.383053","level":"info","event":"at com.amazonaws.internal.EC2ResourceFetcher.doReadResource(EC2ResourceFetcher.java:80)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.383224","level":"info","event":"... 55 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.651831","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.652151","level":"info","event":"File \"/opt/airflow/dags/spark_jobs/split_customer.py\", line 7, in <module>","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.656173","level":"info","event":"df = spark.read.csv(\"s3a://etl-dag/data/retail_data.csv\",","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.656468","level":"info","event":"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.656661","level":"info","event":"File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 740, in csv","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.658089","level":"info","event":"File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.659323","level":"info","event":"File \"/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.660375","level":"info","event":"File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.663480","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o32.csv.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.664099","level":"info","event":": java.nio.file.AccessDeniedException: s3a://etl-dag/data/retail_data.csv: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@72b1251a: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@2a90d44: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.664304","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.664462","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.664581","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3799)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.664734","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.664884","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.664996","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.665104","level":"info","event":"at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.665289","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.665440","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.665549","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.665735","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:756)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.665952","level":"info","event":"at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:754)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.666115","level":"info","event":"at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.666257","level":"info","event":"at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.666465","level":"info","event":"at scala.util.Success.$anonfun$map$1(Try.scala:255)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.666616","level":"info","event":"at scala.util.Success.map(Try.scala:213)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.666763","level":"info","event":"at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.666957","level":"info","event":"at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.667073","level":"info","event":"at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.667206","level":"info","event":"at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.667342","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.667573","level":"info","event":"at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.667699","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668033","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668192","level":"info","event":"at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668339","level":"info","event":"at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668481","level":"info","event":"Caused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DefaultAWSCredentialsProviderChain : com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@72b1251a: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@2a90d44: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668589","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668686","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668805","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.668909","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.669064","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.669291","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.669423","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.669627","level":"info","event":"at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.670120","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.670359","level":"info","event":"at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.670983","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.671272","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6431)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.671481","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.671688","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.671801","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.672027","level":"info","event":"at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.672275","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.672413","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.672540","level":"info","event":"at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.672647","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.672791","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.672891","level":"info","event":"at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.673004","level":"info","event":"... 23 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.673179","level":"info","event":"Caused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from any provider in the chain: [EnvironmentVariableCredentialsProvider: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)), SystemPropertiesCredentialsProvider: Unable to load AWS credentials from Java system properties (aws.accessKeyId and aws.secretKey), WebIdentityTokenCredentialsProvider: You must specify a value for roleArn and roleSessionName, com.amazonaws.auth.profile.ProfileCredentialsProvider@72b1251a: profile file cannot be null, com.amazonaws.auth.EC2ContainerCredentialsProviderWrapper@2a90d44: Failed to connect to service endpoint: ]","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.673376","level":"info","event":"at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:136)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.673495","level":"info","event":"at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.673651","level":"info","event":"... 44 more","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.673868","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.906786","level":"info","event":"25/05/28 10:10:20 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.908410","level":"info","event":"25/05/28 10:10:20 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.934286","level":"info","event":"25/05/28 10:10:20 INFO SparkUI: Stopped Spark web UI at http://localhost:4040","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.951303","level":"info","event":"25/05/28 10:10:20 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.957503","level":"info","event":"25/05/28 10:10:20 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:20.995184","level":"info","event":"25/05/28 10:10:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.021657","level":"info","event":"25/05/28 10:10:21 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.023295","level":"info","event":"25/05/28 10:10:21 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.042097","level":"info","event":"25/05/28 10:10:21 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.050544","level":"info","event":"25/05/28 10:10:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.072844","level":"info","event":"25/05/28 10:10:21 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.073466","level":"info","event":"25/05/28 10:10:21 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.074260","level":"info","event":"25/05/28 10:10:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-5089c052-14bb-455e-b352-860b8727fb89","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.080180","level":"info","event":"25/05/28 10:10:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-5089c052-14bb-455e-b352-860b8727fb89/pyspark-2c02ad6a-0aba-4961-a3a3-5766146fc209","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.085738","level":"info","event":"25/05/28 10:10:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-700b44f9-7556-4705-a72c-46c9283f413c","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.095226","level":"info","event":"25/05/28 10:10:21 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.096504","level":"info","event":"25/05/28 10:10:21 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.096833","level":"info","event":"25/05/28 10:10:21 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook"}
{"timestamp":"2025-05-28T10:10:21.148923","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master spark://spark:7077 --conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain --name arrow-spark --verbose /opt/airflow/dags/spark_jobs/split_customer.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}]}]}
